{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Architecture\n",
    "\n",
    "\n",
    "## Description\n",
    "\n",
    "This concept will introduce you to the need and usage of deep architectures in neural networks. You will also learn about activation functions and implement a neural network using `Keras` library\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Understand the need of deep networks\n",
    "- Activation functions and its different types\n",
    "- Building a deep neural network with `Keras`\n",
    "- Weight initialization variants\n",
    "- Problem of vanishing and exploding gradients\n",
    "\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "- Multi-layer perceptrons\n",
    "- Machine Learning background\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "- Build a deep learning model with `keras`\n",
    "- Use weight initialization startegies and different activations for fine tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Why deep networks?\n",
    "\n",
    "### Description: This chapter emphasizes on the benefit of using deep networks over shallow networks in cases where sufficient data is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "\n",
    "- Need for deeper networks and Need for deep architectures are the same. We need to use a different terminology like motivation for deeper networks in the beginning.\n",
    "- Example not shown properly. Rectify the image.\n",
    "- Matrix representation of network components, the pain point of implementation - understanding the dimensions, is not really clear. Need to elaborate this point further - what do you mean by the dimensions of the network. Also, when you say linear transformation of the inputs, it is not so clear.\n",
    "- General rule of thumb - always overcomment the code. The more you comment the code, the clearer it is to the learner what you are trying to accomplish. Add context to the code, and marry it to the content. Currently, the learner can get lost in the parameter initialisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Shallow vs Deep architectures\n",
    "\n",
    "***\n",
    "\n",
    "**Recap of shallow networks**\n",
    "\n",
    "In the previous concept you must have built a very simple neural network which contained a single hidden layer. These type of networks where the number of hidden layers is less than 3 are generally termed as shallow networks. That's because the network contains less number of hidden layers and subsequently has lesser parameters to train on. Now the main motivation behind using a neural network was to harness its **non-linear** relationship capturing power. A shallow network will definitely serve this purpose but for complex tasks like image classification, voice recognition, object detection, machine translation etc. it has been found that neural networks with more hidden layers are the way to go forward.\n",
    "\n",
    "\n",
    "**What are deep neural networks?**\n",
    "\n",
    "As mentioned previously it is useful to stack more hidden layers in neural networks if the problem demands so. This0 type is also called **deep networks**. A neural network is considered deep if it has more than 3 hidden layers. With deeper networks there comes the added complexity of having more number of parameters (weights and bias) to train but they outperform shallow networks if the data has high non-linearity provided there is sufficient volume of data. **Now it is not advisable to train a deep neural network on small data. Why? Because it leads of overfitting as such networks involve learning a lot of parameters**. The architecture of a typical deep neural network looks like this:\n",
    "\n",
    "<img src=\"../images/dnn.jpeg\">\n",
    "\n",
    "It has an input and output layer just like those in shallow neural networks. In addition to these, it has many hidden layers. While counting layers of a deep neural network, we only count the hidden layers and do not take into account the output layer. Hence the above neural networks has 4 layers.\n",
    "\n",
    "\n",
    "\n",
    "**Need for deep architectures**\n",
    "\n",
    "Deep learning has come into limelight mainly because of the following two reasons:\n",
    "\n",
    "- **Capturing complex non-linear relationships**: By now you must be knowing that shallow neural networks are able to capture simple non-linear trends but unfortunately cannot capture complex non-linearities. This is where deep networks come into the picture. Particularly in tasks like image generation, object detection, machine translation etc. deep networks come in handy as they have been found to model complex relationships well.\n",
    "\n",
    "_An Example_\n",
    "\n",
    "<table>\n",
    "<tr> <td> <img src=../images/task.png style=\"width: 300px\"> </td> <td> <img src=../images/log.png style=\"width: 300px\"> </td> </tr>\n",
    "<tr> <td colspan=2> <img src=../images/a.png style=\"width: 600px\"> </td> </tr>\n",
    "<tr> <td colspan=2> Source: <a href=\"https://github.com/Kulbear/deep-learning-coursera/blob/master/Neural%20Networks%20and%20Deep%20Learning/Planar%20data%20classification%20with%20one%20hidden%20layer.ipynb\">Github</a> </td> </tr> \n",
    "</table>\n",
    "\n",
    "The image above represents a hypothetical distribution where deeper networks were able to model the somewhat petal-shaped decision boundary accurately as compared to both logistic regression and shallow networks.\n",
    "\n",
    "\n",
    "- **Automatic feature extraction**: In traditional Machine learning techniques, most of the features need to be identified by an domain expert. Naturally it took a lot of time to decide on the important features and in most cases hand-engineering them. Deep networks eliminate all of these stuff. The hidden layers in these networks perform the work of feature extractors and capture low-level features. As you go deeper into hidden layers, the features become more high-level.\n",
    "\n",
    "    Consider the case of identifying a car from images: in ML you will manually need to extract features and then feed them to an algorithm which will then classify the image as being of a car or not. But in deep learning you won't be performing any of these steps. You directly feed the image and the network figures out a way to classify and given the right amount of training data it will outperform the traditional ML pipeline\n",
    "\n",
    "<img src=\"../images/feature.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "**Why deep neural networks are widespread nowadays?**\n",
    "\n",
    "The basic technical idea behind deep neural networks have been around for decades but the idea is taking off now. The main drivers behind the rise of deep learning are:\n",
    "- Boom in computational power with the rise of specialized hardware like GPUs and faster networking on many types of hardware for distributed computing\n",
    "- Huge rise in the amount of data availability. Eg. Time spent on internet has increased significantly, mobile phones with cameras have lead to creation of large image datasets, continuous logging of data from internet of things devices\n",
    "- Advances in convex optimization algorithms leading to guaranteed covergence of models\n",
    "\n",
    "If we plot a figure where on the horizontal axis we plot the amount of data we have for a task and on the vertical axis we plot the performance on learning algorithms, we observe that for traditional learning algorithm the curve is increasing for a while as you add more data but after a while the curve flattens. On the other hand, for deep neural networks, the performance keeps on increasing as you keep on adding more data, the performance keeps on increasing. Hence, deep learning really shines when there is huge amount of data\n",
    "<img src=\"../images/graph.jpeg\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Deep Neural Network architecture and terminologies\n",
    "\n",
    "***\n",
    "\n",
    "Now lets get formally acquainted with the terminologies of a deep neural network. The basic structure is similar to that of a MLP albeit with the presence of additional hidden layers.\n",
    "\n",
    "We will take this image for illustration: <img src=\"../images/deep_nn.png\"> It is a fully connected (every neuron/node is connected to every other neuron/node) with three hidden layers. \n",
    "\n",
    "The important terms are:\n",
    "- **Nodes**: The small colored circles are referred to as neurons/nodes. They are responsible for storing information and are considered workhorses of Artificial Neural Networks (ANNs).\n",
    "- **Layers**: A Neural Network consists of neurons stacked layerwise such that every neuron from the input layer is connected to every neuron from the subsequent/next layer, but neurons of the same layer are not interconnected. Our network has layers $L_1$, $L_2$, $L_3$, $L_4$ and $L_5$.\n",
    "- **Input Layer**: This is the very first layer of any Neural Network that brings the initial data into the system for further processing by subsequent layers of artificial neurons/nodes. The layer $L_1$ (colored green) is the input layer with inputs $x_1$, $x_2$, ..., $x_p$.\n",
    "- **Output Layer**: It is the last layer of the network that is responsible for giving the output. The layer $L_5$ (colored red) is the ouput layer with outputs $y_0$, $y_1$, $y_2$, ..., $y_9$.\n",
    "- **Weights**: Every node to node arrow represents a numeric value called weight. Goal of training a neural network is to update this weight value to decrease the loss(error). In the figure the connections between the $L_1$ and $L_2$ represent weights which is given by the wright matrix $W^{(1)}$. Similarly $W^{(2)}$, $W^{(3)}$ and $W^{(4)}$ represent weights for the subsequent layer connections.\n",
    "- **Activation function**: In laymen terms, activation functions convert input signals of a node to an output signal. That output signal now is used as an input in the next layer in the stack. In ANN we do the sum of products of inputs and their corresponding weights($W^{T}x$) and apply an activation function $f(x)$ to it ($f(W^{T}x)$) to get the output of that layer and feed it as an input to the next layer. In the image $a^{(2)}$, $a^{(3)}$ etc are the matrix representations of activation function.\n",
    "- **Bias**: A bias can be considered a special weight whose input has a constant value of $1$. Like weights, bias values are learnt during training a neural network. \n",
    "- **Hidden Layers**: Every layer sandwiched between input and output layer is called a hidden layer. More hidden layers mean the network can capture more complex functions. Here, neurons take in a set of weighted inputs and produce an output through an activation function. The layers $L_2$, $L_3$ and $L_4$ (colored blue) are hidden layers.   \n",
    "\n",
    "\n",
    "**Notations of deep networks**\n",
    "\n",
    "From the previous concept on MLP, you should be familiar with the topics of forward and backward propagation. But you need to initialize weight matrices before caculating the outputs required for forward propagation. We will be introducting some notations that will be used throughout this entire concept:\n",
    "- $\\mathbf{J}$: Cost function \n",
    "- $\\mathbf{w^{(l)}}$, $\\mathbf{b^{(l)}}$: Weight and bias matrix for the $l^{th}$ layer\n",
    "- $\\mathbf{z^{(l)}}$: Linear transformation of inputs for the $l^{th}$ layer\n",
    "- $\\mathbf{g^{(l)}}$: Activation function on the $l^{th}$ layer\n",
    "- $\\mathbf{a^{(l)}}$: Activation function output for the $l^{th}$ layer\n",
    "- $\\mathbf{dw^{(l)}}$, $\\mathbf{db^{(l)}}$: Derivative of cost function with respect to $w^{(l)}$ :$\\mathbf{\\frac{\\partial J}{\\partial w^{(l)}}}$ and $b^{(l)}$: $\\mathbf{\\frac{\\partial J}{\\partial b^{(l)}}}$\n",
    "- $\\mathbf{dz^{(l)}}$: Derivative of cost function with respect to $z^l$:$\\mathbf{\\frac{\\partial J}{\\partial z^{(l)}}}$\n",
    "- $\\mathbf{da^{(l)}}$: Derivative of cost function with respect to $a^{(l)}$:$\\mathbf{\\frac{\\partial J}{\\partial a^{(l)}}}$\n",
    "- $\\mathbf{n^{(l)}}$: Number of nodes in the $l^{th}$ layer\n",
    "- $\\mathbf{m}$: Number of training examples\n",
    "- $\\mathbf{L}$: Number of layers in the network\n",
    "\n",
    "\n",
    "One of the major pain points in implementing a deep neural network is getting the right dimensions. For this we have decided to implement some standard notations as well which you should be following throughout. Now lets look at what the dimensions:\n",
    "- $\\mathbf{w^{(l)}}$, $\\mathbf{dw^{(l)}}$: Number of nodes in $l^{th}$ layer x Number of units (nodes) in ${(l−1)}^{th}$ layer\n",
    "- $\\mathbf{b^{(l)}}$, $\\mathbf{db^{(l)}}$: Number of nodes in $l^{th}$ layer x 1 \n",
    "- $\\mathbf{a^{(l)}}$, $\\mathbf{da^{(l)}}$, $\\mathbf{z^{(l)}}$, $\\mathbf{dz^{(l)}}$: Number of nodes in $l^{th}$ layer x numer of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. Deep Learning eliminates the need for feature engineering.\n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (a) YES\n",
    "\n",
    "**Explaination**: With deep learning there is no need for feature engineering\n",
    "\n",
    "\n",
    "2. Deeper the network, the more non-linear relationships will be captured by neural networks.\n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (a) YES\n",
    "\n",
    "**Explaination**: When network gets deeper, the ability to learn complex relationships increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Activation Functions\n",
    "\n",
    "### Description: This chapter deals with the importance of activation functions and its different types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Need for activation functions\n",
    "\n",
    "***\n",
    "\n",
    "The previous concept on **Multi-layer Perceptrons** must have introduced you to activation functions and its need especially in context with deep neural networks. This topic is meant to be a refresher on activation functions.\n",
    "\n",
    "\n",
    "**What are activation functions?** \n",
    "\n",
    "Activation functions are non-linear functions which are applied to outputs of each layer for getting non-linearity in the output. If we did not use activation functions, then we would always end up with a linear model. (Linear combination of linear functions is a linear function)\n",
    "\n",
    "\n",
    "**Why use activation functions?**\n",
    "\n",
    "As already mentioned before, most of the real world problem statements involve capturing complex non-linear relationships. Activation functions introduce non-linearity to an otherwise linear function. For example: Sigmoid function is a popular activation function. Other activations include tanh, ReLU, Leaky ReLU etc. and you will learn more about them in the next topic.\n",
    "\n",
    "\n",
    "<img src=../images/Logistic-curve.svg style=\"width: 250px;\">\n",
    "\n",
    "\n",
    "\n",
    "**Consequences of not using activation function**\n",
    "\n",
    "We will look at how not using activation functions can prevent us from obtaining non-linear decision boundaries, both from a mathematical point of view and from the point of view of a dataset.\n",
    "\n",
    "\n",
    "- ***`MATHEMATICAL EXPLAINATION`***\n",
    "\n",
    "    We mentioned the consequence of not using activation function. Lets see in a simple neural network what happens if we do not use activation function. Suppose you have a two-layer perceptron network with one hidden layer where the hidden layer consists of two neuron units. \n",
    "\n",
    "<img src='../images/nn.png'> \n",
    "\n",
    "   We look at the overall output of the network when the activations in the hidden units are linear:\n",
    "\n",
    "- Output of the hidden unit $h_1 = w_{11}x_1 + w_{21}x_2 + b_1$\n",
    "- Output of the hidden unit $h_2 = w_{12}x_1 + w_{22}x_2 + b_2$\n",
    "- Output of the output unit $p_1 = w_1(w_{11}x_1 + w_{21}x_2 + b_1) + w_2(w_{12}x_1 + w_{22}x_2 + b_2) + b_3\n",
    "= (w_1w_{11} + w_2w_{12})x_1 + (w_1w_{21} + w_2w_{22})x_2 + w_1b_1 + w_2b_2 + b_3$\n",
    "\n",
    "    As you can see the output $p_1$ is a linear function. The major drawback of simple perceptron was that it could not catch non-linear relationships (like XOR) and for thus reason MLPs came into being with activation functions. Now, as you have seen above, disabling the activations produced a linear output which would again fail to identify a non-linear pattern.\n",
    "\n",
    "\n",
    "- ***`INTUITIVE EXPLAINATION`***\n",
    "\n",
    "    Consider you have the following distribution of data where the two classes are represented by green and red colors. From the picture it is evident that the decision boundary is non-linear. \n",
    "    <img src='../images/data.png'>\n",
    "    \n",
    "    On applying a neural network with single hidden layer with 3 neurons without activation we get the following image. The smaller dots (both red and green) are those data points which have been predicted correctly. The larger dots (both red and green) are data points which have been correctly classified. **Clearly the model is unable to identify the top cluster of points as green and instead predicting those points as red**. In other words the model learnt a linear decision boundary which identifies the upper points as being **red** and the lower points as **green**.\n",
    "    \n",
    "    <img src='../images/sigmoid.png'>\n",
    "    \n",
    "    On using a single layer neural network with 3 neurons and sigmoid activation we obtain the following decision boundary. Observe how the number of bigger dots (both green and red) have decreased drastically. In other words, the model was now able to model the non-linear decision boundary.\n",
    "    \n",
    "    <img src='../images/nn_arc.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Different activation functions\n",
    "\n",
    "***\n",
    "\n",
    "So you now know the need for using activation functions. There are many such functions and their usage varies with respect to the problem at hand. We will be discussing all the relevant activation functions used nowadays.  \n",
    "\n",
    "- **`Linear activation (Identity)`**: This type maintains the identity of the input to the neurons where the *input is a weighted linear combination of the values coming from the previous layer* i.e. if $x_1$, $x_2$ and $x_3$ are the inputs and $w_1$, $w_2$ and $w_3$ are the weights then the ouput will be simply $y = c(w_1x_1 + w_2x_2 + w_3x_3)$ where $c$ is a constant. It is essentially about preserving the original linear combination and scaling it by a constant. \n",
    "    \n",
    "    But it is rarely used because of the obvious fact that it fails to produce non-linear decision boundaries. That means these N layers can be replaced by a single layer. We just lost the ability of stacking layers this way and no matter how we stack, the whole network is still equivalent to a single layer with linear activation (a combination of linear functions in a linear manner is still another linear function). \n",
    "\n",
    "<img src='../images/linear.png'>\n",
    " \n",
    " \n",
    "- **`Binary Threshold (Step)`**: Probably the most intuitive of all activation functions is the step function. First you decide a threshold beyond which the neuron will fire. Mathematically, if $w$, $x$, $b$, $k$ are the weights, inputs, biases and threshold respectively then, \n",
    "\n",
    "$\\text{If } z = w^Tx + b$,  $\\mathbf{y = 1} \\text{ if } z > k$ and $\\mathbf{y = 0} \\text{ if } z <= k$ \n",
    "\n",
    "But there are certain drawbacks of using step function as activation functions. Firstly, it works as a binary classifier but will be more difficult to implement (not impossible) for more than two classes. Secondly, the derivative is zero for this function and hence during backpropagation there is no weight update and the model learns nothing. \n",
    "\n",
    "<img src='../images/step.png'>\n",
    "\n",
    "\n",
    "- **`Sigmoid`**: This function squishes values between 0 and 1. Mathematically it is given as $ f(x) = \\frac1{1+e^{-x}} $. Its main advantage over linear and step activations is that it is capable of producing non-linear decision boundaries. If you look at the image of sigmoid below, you can notice that it has a smooth curve and also gives nice continuous derivatives. Its ouput is between 0 and 1 and for this reason it is used to depict probabilities with respect to a given class for binary classification. Also, it won't blow up activation values like linear activation since its output is in the range $[0, 1]$.\n",
    "\n",
    "   Well what about its derivative then? \n",
    "   \\begin{align}\n",
    "    \\dfrac{d}{dx} f(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\\n",
    "    &= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\\n",
    "    &= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\\n",
    "    &= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\\n",
    "    &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\\\\n",
    "    &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\\\\n",
    "    &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "    &= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "    &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "    \\end{align}\n",
    "   If you notice in the image, the derivative (red dotted line), between X values -3 to 3, Y values are very steep. Which means, any small changes in the values of X in that region will cause values of Y to change significantly. This means that sigmoid has a tendency to bring the Y values to either end of the curve. It is a desirable property for a binary classifier as it tends to bring the activations to either side of the curve (above $x = 3$ and below $x = -3$ for example). \n",
    "   \n",
    "   But there is a significant problem with its derivative after $x = 3$ and before $x = -3$. If you observe the red dotted line, the gradient becomes less and less and due to the chaining effect in backpropagation the weight update might become so small that the network refuses to learn anything. This is commonly referred to as the **vanishing gradient problem**. \n",
    "\n",
    "<img src=\"../images/der_sigmoid.png\">\n",
    "\n",
    "    \n",
    "- **`Softmax`**: With sigmoid you can perform binary classification but what about multi-class classifications? This is where softmax activation comes into the picture and is usually applied at the last layer. It is a generalization of the sigmoid function where if there are $k$ output classes and the weight vector for the ith class is $w^{i}$, then the predicted probability for the $i^{th}$ class given the input vetor $x \\in \\mathcal{R}^n$  is given by:  $$P(y_i = 1 / x) = \\frac{e^{w^{(i)T}x + b^{(i)}}}{\\sum\\limits_{j=1}^{k}{e^{w^{(j)T}x + b^{(j)}}}}$$. \n",
    "\n",
    "    Here the probability of the positive class for a two-class softMax has the same expression as that of a sigmoid activation function, the only difference being that in the sigmoid we only use one set of weights, while in the two-class softMax there are two sets of weights.  \n",
    "\n",
    "    The loss function with softmax output is generally taken as the categorical cross entropy which is given by the expression: $$\\text{Loss } = \\sum\\limits_{i=1}^{k}{-y_i\\log{P(y_i=1/x)}}$$ \n",
    "    \n",
    "    <img src='../images/softmax.png'>\n",
    "\n",
    "\n",
    "- **`Tangant Hyperbolic (Tanh)`**: If you compare the graphs of sigmoid and tanh you will find them having  exactly similar shapes except for the fact that **tanh is zero-centred while sigmoid is not**. Having zero-centred inputs means that inputs can be both positive and negative with tanh activations. But what is the problem with having only positive or negative inputs? Well, it turns out that having inputs of only one sign will produce gradients of only one sign , due to which weight updates will follow undesirable zig-zag patterns. However, notice that once these gradients are added up across a batch of data, the final update for the weights can have variable signs, somewhat mitigating this issue. Therefore, this is an inconvenience but it has less severe consequences compared to the vanishing gradient problem.  \n",
    "\n",
    "    TanH activation squishes outputs in the range $[0,1]$ so it has no chances of blowing up gradients. Mathematically it is given as $$y = \\frac{e^{-z} - e^{x}}{e^{-z} + e^{x}}$$ $$\\text{where } z = w^Tx + b$$\n",
    "    \n",
    "    Like sigmoid it also suffers from the vanishing gradient problem as is evident from the graph below (gradients saturate for $x < -2$ and $x > 2$). Another thing to notice is that its gradient is slightly steeper than that of sigmoid and so deciding between these two activations is really a matter of preference on whether you want steeper gradients or not. Also, TanH is actually a scaled version of sigmoid and can be written as: $$\\tanh(x) = 2\\sigma(2x) - 1$$\n",
    "\n",
    "<img src=\"../images/tanh.png\">\n",
    "\n",
    "\n",
    "- **`ReLU (Rectified Linear Unit)`**: Perhaps the most popular and widely used activation function is the ReLU function. If the net input is greater than 0 then it preserves the original net input and outputs 0 if the net input is less than or equal to 0. Mathematically it is given as $$y = \\max(0, w^{T}x + b)$$ <img src=\"../images/Relu.png\"> \n",
    "\n",
    "    ReLU is one of the many elements that played a key role in revolutionalizing deep learning. Some of the pros and cons of using ReLU are as follows:\n",
    "  - It helps the optimizer in finding the right set of weights sooner. More technically it makes the convergence of stochastic gradient descent faster (e.g. a factor of 6 in [Krizhevsky et al.](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)) as compared to sigmoid and tanh. It is argued that this is due to its linear, non-saturating form.\n",
    "  - It does not suffer from the vanishing gradient problem due to its constant gradient (equal to 1) for positive inputs thereby ensuring that the network doesn't stop learning. So, during backpropagation updates do not get smaller but remain the same (if input is positive) ensuring that learning does not stop.\n",
    "  - It is computationally inexpensive as compared to sigmoid and tanh as it has a constant gradient when input is positive and also ReLU outputs are cheap to compute.\n",
    "  - ReLU results in sparse networks (output is 0 when input is negative). How this helps? Well, it turns out that in such a scenario its more likely that neurons are actually processing meaningful aspects of the problem. For example, in a model detecting cats in images, there may be a neuron that can identify ears, which obviously shouldn’t be activated if the image is about a building.\n",
    "  - ReLU results in sparse networks, and such networks are faster than a dense network, as there are fewer things to compute and process.\n",
    "  - The output of ReLU is between $[0,\\infty]$, so it has no upper bound which can lead to blowing up of activation inputs. \n",
    "  - The horizontal orange line(denoting gradient of ReLU) doesn't let the weights update i.e. neurons become non-responsive. Ultimately it may lead to a large portion of the network becoming passive. It is called as the **Dying ReLU** problem and we will address this issue more in an upcoming topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. Sigmoid activation suffers from the problem of vanishing gradients\n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (a) YES\n",
    "\n",
    "**Explaination**: Sigmoids value is between 0 and 1. So, when it is raised to powers above 1 its gradient might become too small resulting in vanishing gradients\n",
    "\n",
    "\n",
    "2. What will be the output of $20x - 25$ for $x=1$ with ReLU activation?\n",
    "\n",
    "    (a) 0\n",
    "    \n",
    "    (b) 5\n",
    "    \n",
    "**ANS**: (a) 0\n",
    "\n",
    "**Explaination**: At $x=1$, value is $20*1 - 25 = 20 - 25 = -5$. With ReLU activation this will be 0 as ReLU converts any negative input to zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Deep Learning with Keras\n",
    "\n",
    "### Description: This chapter will introduce you to the keras package and how you can use it to perform deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "- Check the notation and correct the mistakes in forward propagation.\n",
    "- For the pseudo code, always provide a context in form of a comment/observation. Let the explanation come first and then the code.\n",
    "- Abstract the back propagation math and give it as part of a deep dive. Give the bare min needed for explanation.\n",
    "- It is the first time Keras is introduced, give a little more context and explanation of Keras.\n",
    "- The features of music genre classification are not explained properly. The tasks are not defined properly.\n",
    "- Clipping - can we have an illustration to explain it better.\n",
    "- Identify videos that can augment and improve the understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Forward-propagation and Backpropagation refresher\n",
    "\n",
    "***\n",
    "\n",
    "#### FORWARD PROPAGATION\n",
    "\n",
    "This topic is meant to be a refresher on forward and backward propagation. In the previous concept on **Multi-Layer Perceptron**s forward and backpropagation were covered in-depth. **Forward propagation in deep neural networks works in exactly the same way as you saw with MLPs**. It refers to the process of proceeding forward in a network calculating and storing intermediate values (linear combinations, activations etc.) in the order from input to output layer. Various linear transformations and non-linearities are applied to the data in steps to generate the predicted values. Specifically, each layer corresponds to a linear tranformation $z = w X + b$ followed by a non-linear activation function $a = \\text{activation}(z)$ where\n",
    "\n",
    "$m$ : numbers of data samples \n",
    "\n",
    "$\\mathbf{X}$ : input matrix with the individual data points stacked vertically having $m$ columns\n",
    "\n",
    "$n[l]$ : number of neurons in the $l^{\\text{th}}$ layer\n",
    "\n",
    "$\\mathbf{w}^{(l)}$ : matrix of weights corresponding to layer $l$ having size $n[l] \\times n[l-1]$\n",
    "\n",
    "$b^{(l)}$ : bias corresponding to $l^{\\text{th}}$ layer\n",
    "\n",
    "$\\mathbf{z}^{(l)}$ : output of the $l^{\\text{th}}$ before applying the activation function\n",
    "\n",
    "$\\mathbf{a}^{(l)}$ : output of the $l^{\\text{th}}$ after applying the activation function \n",
    "\n",
    "Also, consider $\\mathbf{a}^{(0)} = \\mathbf{X}$ for the sake of simplicity\n",
    "\n",
    "\n",
    "**Pseudocode for forward propagation**\n",
    "\n",
    "The forward propagation algorithm works as follows:\n",
    "\n",
    "```python\n",
    "# iterate over number of layers\n",
    "for i in [1...l-1]\n",
    "   do\n",
    "    # compute linear combination of weights and inputs\n",
    "    z[i] = w[i] a[i-1] + b[i]\n",
    "    \n",
    "    # compute activations\n",
    "    a[i] = activation(z[i])\n",
    "```\n",
    "\n",
    "**Explaination of pseudocode**:  \n",
    "\n",
    "- First, a linear transformation is applied on the inputs $\\mathbf{X}$ to get $\\mathbf{Z}^{(1)}$\n",
    "\n",
    "$$ \\mathbf{z}^{(1)} = \\mathbf{w}^{(1)} \\mathbf{X} + b^{(1)} $$\n",
    "\n",
    "- Then a non-linearity is applied to $\\mathbf{z}^{(1)}$ in the form of an activation function (say sigmoid) to get the activations for the 1st layer $\\mathbf{a}^{(1)}$\n",
    "\n",
    "$$ \\mathbf{a}^{(1)} = \\sigma \\left( \\mathbf{z}^{(1)} \\right)$$\n",
    "\n",
    "- Now, a linear transformation is applied to $\\mathbf{a}^{(1)}$ to get $\\mathbf{z}^{(2)}$\n",
    "\n",
    "$$ \\mathbf{z}^{(2)} = \\mathbf{w}^{(2)} \\mathbf{a}^{(1)} + b^{(2)} $$\n",
    "\n",
    "- Then a non-linearity is applied to $\\mathbf{z}^{(2)}$ in the form of an activation function (say sigmoid) to get the activations for the 2nd layer $\\mathbf{a}^{(2)}$\n",
    "\n",
    "$$ \\mathbf{a}^{(1)} = \\sigma \\left( \\mathbf{z}^{(2)} \\right)$$\n",
    "\n",
    "This is repeated for all the layers of the neural network except till we reach the end of the network\n",
    "\n",
    "\n",
    "#### BACKWARD PROPAGATION\n",
    "\n",
    "It is the method by which we find the best set of weights incrementally using an iterative algorithm (gradient descent, stochastic gradient descent etc.) by comparing our predictions with the actual targets.\n",
    "\n",
    "Consider a network with single hidden layer. Then first $z^{(1)}$ is calculated, then $a^{(1)}$ till $a^{(2)}$ which predicts our class (say). Now the loss (J) is evaluated which is some measure of difference between true target and our prediction $a^{(2)}$. Now if you want to compute new weight $w_1$ after an iteration, you use the backpropagation algorithm to compute $dw^{(l)}$. The below flow chart gives you an accurate representation of forward and backprop together. \n",
    "\n",
    "<img src=../images/backprop.png>\n",
    "\n",
    "$$ dw^{(l)} = \\frac{\\partial \\mathcal{J} }{\\partial w^{(l)}} = \\frac{1}{m} dz^{(l)} a^{(l-1) T}$$\n",
    "$$ db^{(l)} = \\frac{\\partial \\mathcal{J} }{\\partial b^{(l)}} = \\frac{1}{m} \\sum_{i = 1}^{m} dz^{(l)(i)}$$\n",
    "$$ da^{(l-1)} = \\frac{\\partial \\mathcal{J} }{\\partial a^{(l-1)}} = w^{(l) T} dz^{(l)}$$\n",
    "\n",
    "\n",
    "After calculating these terms, you will apply the weight update rule. But but isn't it cumbersome writing down these equations and then converting them into code snippets everytime you decide to build a deep network to solve your problem? In the next topic you will be introduced to the `keras` library which kind of abstracts out the mathematical details and allows the practioner to focus on key things like chosing the number of hidden layers, neurons in every layer etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Introduction to `keras`\n",
    "\n",
    "***\n",
    "\n",
    "**Need for deep learning packages**\n",
    "\n",
    "You coded up an MLP from scratch in the previous concept. It was very time-consuming and you may want to modify it if something extra is needed out of it. Moreover, from resuability point of view it is not advised to write things from scratch as those need to be tested out every time during production. This is where deep learning libraries like `Keras`, `TensorFlow`, `PyTorch` etc. come into the picture. Specifically if you're familiar with Python's `scikit-learn` package (used for machine learning) then `keras` would be extremely easy for you.\n",
    "\n",
    "`Keras` provides you with a lot of funtionality and you can use it to train, test, tune hyperparameters as well as generate plots. Besides that it is very easy to use for beginners and also robust in production. \n",
    "\n",
    "\n",
    "**Gentle introduction to `keras`**\n",
    "\n",
    "Lets get started with it then. First we will give you the code and then explain what every line is doing. Keras is a minimalist, highly modular deep neural network library written in Python and built on top of Theano and Tensorflow. This allows it to accelerate computations by offloading them to one or more GPUs. It is also easier to use than raw Theano and Tensorflow, as it includes several functionalities such as:\n",
    "\n",
    "- Standard APIs for sequential and functional models with feedforward, convolutional, and recurrent layers\n",
    "- Several optimization algorithms and advanced activation functions\n",
    "- Monitoring/callback support\n",
    "- Easily extensible\n",
    "- Simple models, such as feedforward networks, are straightforward to implement. Have a look at the below code:\n",
    "\n",
    "```python\n",
    "# import packages\n",
    "import numpy as np\n",
    "np.random.seed(42)  # for reproducibility\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "# build a neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "Only these are the lines of code you require to spin up your own deep neural network. Isn't it cool! Now lets get on the details of the important lines of code:\n",
    "\n",
    "***Note that the last layer has 10 neurons; which essentially means that the snippet is meant to tackle a multiclass problem. For binary classification tasks you can take either single output neuron with sigmoid activation or two neurons with softmax activation. For regression tasks, you will require only one output neuron.***\n",
    "\n",
    "- **`model = Sequential()`**: Sequential models are stacks of layers, where the output of the previous layer is the input of the current layer. It makes use of the Sequential API available in `keras` which is also the most widely used API. Apart from Sequential API, `keras` also has another API called ***Functional*** API which allows one to build more complex deep learning models. You can read more about this in the [official documentation page](https://keras.io/getting-started/functional-api-guide/).\n",
    "\n",
    "- **`model.add()`**: Adds a layer to the model. For the first layer one should specify the number of input features with the argument `input_shape` or with `input_dim`. In the code snippet the argument `input_shape=(784,)` simply means that you have $784$ features. You could have also used `input_dim=784` to represent the same fact. **After the addition of the input layer, the following layers can do automatic shape inference**. This is indeed one big benefit!\n",
    "\n",
    "- **`Dense()`**: A Dense layer simply performs an affine transform $z = wx + b$. The number $512$ denotes the number of neurons in that dense layer.\n",
    "\n",
    "- **`Activation()`**: Applies elementwise non-linearity function to the output of the previous layer: $a = g(z)$. Although the code snippet has a separate line of code to perform it; you do not have to add it separately: the initializer for most layers in Keras, including Dense, has a keyword argument `activation` by which you can specify the activation function.\n",
    "\n",
    "- **`model.summary()`**: Gives you useful information like the number of layers, number of neurons and most importantly the number of trainable parameters which are nothing but the weights and biases\n",
    "\n",
    "The output of the above code snippet is:\n",
    "\n",
    "```python\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 512)               401920    \n",
    "_________________________________________________________________\n",
    "activation_1 (Activation)    (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 512)               262656    \n",
    "_________________________________________________________________\n",
    "activation_2 (Activation)    (None, 512)               0         \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 10)                5130      \n",
    "_________________________________________________________________\n",
    "activation_3 (Activation)    (None, 10)                0         \n",
    "=================================================================\n",
    "Total params: 669,706\n",
    "Trainable params: 669,706\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "\n",
    "**Introduction to the problem statement**\n",
    "\n",
    "The problem you will be working on is churn prediction which is a binary classification task (the same dataset in the previous concept of MLP). You have a dataset of a bank with $10000$ customers. The dataset contains lots of attributes of the customers measured over last 6 months such as `name`, `credit score`, `geography`, `age`, `tenure`, `balance` etc. Also, you know which of these customers stayed and left. Now, given a new customer profile, your task is to create a model out of this data that can predict whether an incoming new customer will **leave** or **stay**. \n",
    "\n",
    "\n",
    "**Why solve this?**\n",
    "\n",
    "This problem is valuable to any customer-oriented organisations, e.g. *Should this person get a loan or not?* *Is this a fradulent transaction?* Therefore, **Customer Churn Modelling** finds its applications in varied sectors. Its used by telecom service providers, banks to name a few.\n",
    "\n",
    "\n",
    "**Brief Explanation of Dataset and Features**\n",
    "- `CreditScore(Discrete)`: Denotes the credit rating of the customer given by the bank \n",
    "- `Gender(Categorical)`: Gender of the customer\n",
    "- `Age(Discrete)`: Age of the customer\n",
    "- `Tenure(Discrete)`: Number of years spent with the organisation\n",
    "- `Balance(Continuous)`: Account balance of the customer\n",
    "- `NumOfProducts(Discrete)`: Number of products of the customer\n",
    "- `HasCrCard(Categorical)`: Does the customer have a credit card?\n",
    "- `IsActiveMember(Categorical)`: Is the customer an active member?\n",
    "- `EstimatedSalary(Continuous)`: Estimate of the salary of the customer\n",
    "- `Exited(Categorical)`: Has the customer exited from the organisation\n",
    "\n",
    "\n",
    "**What do we want as outcome?**\n",
    "\n",
    "Given the features about the Customer, we want to predict if the customer has exited from the organisation with the help of neural networks. You will be using `keras` to build one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your first `keras` deep learning model\n",
    "\n",
    "In this task you will build the neural network model required for the task. Data preprocesing has already been done for you so that you can focus more on the model building aspect. `X_train`, `X_test`, `y_train` and `y_test` represent the training features, test features, training target anf test target respectively\n",
    "\n",
    "\n",
    "### Instructions\n",
    "- Instantiate a neural network `model` using `model = Sequential()`\n",
    "- Add $64$ neurons with input shape `(9,)` using `.add(Dense())`\n",
    "- Add relu activation using any of the methods described in the topic\n",
    "- Now add two hidden layers with relu activation with $256$ and $32$ neurons respectively\n",
    "- The final output layer will have two neurons since this is a binary classification task. The activation function is `'softmax'`\n",
    "- Use `.summaru()` method of `model` to display the neural network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                640       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 25,570\n",
      "Trainable params: 25,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# file path\n",
    "path = '../data/Churn_Modelling.csv'\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# separate into features and target\n",
    "X = data[[\"CreditScore\", \"Age\", \"Tenure\", \"Balance\", \"NumOfProducts\", \"HasCrCard\", \"IsActiveMember\", \"EstimatedSalary\"]]\n",
    "y = data[\"Exited\"]\n",
    "\n",
    "# mean normalization and scaling\n",
    "mean, std = np.mean(X), np.std(X)\n",
    "X = (X - mean) / std\n",
    "X = pd.concat([X, pd.get_dummies(data[\"Gender\"], prefix=\"Gender\", drop_first = True)], axis = 1)\n",
    "\n",
    "# transform data according to the model input format\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .20, random_state= 9, stratify=y)\n",
    "\n",
    "# one-hot encode labels\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# build a neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(9,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training model with `keras`\n",
    "\n",
    "***\n",
    "\n",
    "Up until now you have built out only the architecture i.e. defined how many hidden layers will be there, the number of neurons in every layer, size of the input and output layer, activation functions etc. But you haven't trained your model yet. Lets see how can perform training with `keras`. \n",
    "\n",
    "\n",
    "**Compilation of model**\n",
    "\n",
    "Since `keras` is based on either Theano or Tensorflow backend, they allow you to define mathematical expressions symbolically and get an optimized implementation of the expressions and its gradients **for free**. However, that means models have to be compiled prior to training/inference, as the symbolic expressions need to be converted to *C++/CUDA* code and compiled. Your Python code will automatically interact with this compiled code, so you do not need to worry about interfacing. [Read here](https://keras.io/models/model/#compile) if you want to have a thorough understanding of the compilation process in keras.\n",
    "\n",
    "Below is the python code for carrying out model compilation with `keras`:\n",
    "\n",
    "```python\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "When compiling a model, you need to pass at least two other elements:\n",
    "\n",
    "- **`loss`**: Loss function/Cost function that will be optimized during training. Here, `'categorical_crossentropy'` was the loss function\n",
    "- **`optimizer`**: An optimizer algorithm, which will be responsible for updating the model after each iteration. Here it is `'sgd'` i.e. Stochastic Gradient Descent but there are a lot of other optimizers as well and you will learn about them in-depth in the next concept on Optimizing Neural Networks.\n",
    "- **`metrics`**: You can also optionally pass a number of metrics that will be evaluated at each iteration, but that are not used in the cost function. In this case, we want to know the accuracy of the optimizer.\n",
    "\n",
    "\n",
    "**Trainig with `.fit()` method**\n",
    "\n",
    "After compilation of the model you are ready to start off with training. Below given is a snippet of training with `keras`:\n",
    "\n",
    "```python\n",
    "# batch-size per epoch\n",
    "batch_size = 128\n",
    "\n",
    "# number of epochs\n",
    "nb_epoch = 10\n",
    "\n",
    "# model training\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_val, Y_val))\n",
    "```\n",
    "\n",
    "***Epoch and batch-size***\n",
    "\n",
    "A particular word that you will be coming across frequently in deep learning is **epochs**. But first lets discuss about batch-size. **`batch_size` denotes the number of training examples to be considered for updating the model parameters**. In this case there are a total of $128$ samples in every batch which means the training set will consist of $128$ data points. **Epochs** is the number of passes over the training dataset that you will perform; which means you will be updating your parameters equal to the number of epochs. In a nutshell your algorithm will do this\n",
    "- Take a training batch equal to the number of samples\n",
    "- Update the weights at the end of the training of first batch\n",
    "- Go to the next batch witht the same number of samples\n",
    "- Update weights again\n",
    "\n",
    "This continues till the specified number of epochs. In the snippet above the number of epochs is $10$.\n",
    "\n",
    "Now time to train the model. You do it with `.fit()` method of `model`. It takes arguments the training features (`X_train`), training target (`Y_train`), the batch size (`batch_size`), number of epochs (`nb_epoch`), validation data (`validation_data`) etc. You can read more about the hyperparmeters [here](https://keras.io/models/sequential/#fit)\n",
    "\n",
    "\n",
    "**Callbacks**\n",
    "\n",
    " While training a model, you often want to perform certain operations whenever certain events happen. For example, the last epcoh may not necessarily have the optimum weights, so you want the optimum weights. Or you want to stop training if loss does not decrease after lets say 2 epochs. `keras` supports several different **callbacks** that do things such as save your training history, save the model after each epoch (or whenever there is an improvement), and early stopping. Typical usage is as follows:\n",
    "\n",
    "```python\n",
    "# import packages\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# save best model\n",
    "save_best = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "# early stopping\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(..., callbacks=[save_best, early_stop])\n",
    "```\n",
    "\n",
    "The arguments `callbacks` in the `.fit()` method is doing two things:\n",
    "- Saving the best model with the best set of model parameters with `save_best`. It is basically a kind of checkpointing your model\n",
    "- Checking if loss is decreasing. If it is not decreasing until 5 iterations, then your training will terminate and the optimum parameters are returned for you. This is done with the variable `early_stop`. \n",
    "\n",
    "\n",
    "**Plotting loss curves**\n",
    "\n",
    "Remember the `history` variable where you had stored the training process. Well it has a nice utility. You can plot validation curves i.e. loss curves with number of epochs. How to do it? First lets observe the output of this code snippet:\n",
    "\n",
    "```python\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "```\n",
    "It returns\n",
    "```python\n",
    "['acc', 'loss', 'val_acc', 'val_loss']\n",
    "```\n",
    "\n",
    "Now you can plot the validation curves using the below snippet:\n",
    "```python\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "It returns the following two images:\n",
    "\n",
    "<img src='../images/val_2.png'>\n",
    "\n",
    "<img src='../images/val_1.png'>\n",
    "\n",
    "The plots can provide an indication of useful things about the training of the model, such as:\n",
    "\n",
    "- Speed of convergence over epochs (slope).\n",
    "- If the model may have already converged (plateau of the line).\n",
    "- If model is over-learning the training data (inflection for validation line).\n",
    "\n",
    "\n",
    "**Visualizing the neural network**\n",
    "\n",
    "How about visualizing the network that you had designed? Lets see how you can perform it with the help of a package called `ann_visualizer`.\n",
    "\n",
    "```python\n",
    "# import package\n",
    "from ann_visualizer.visualize import ann_viz\n",
    "\n",
    "# visualize network\n",
    "ann_viz(model, title=\"Artificial Neural network - Model Visualization\")\n",
    "```\n",
    "\n",
    "It gives a PDF file as output and looks somewhat like this. <img src='../images/viz.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model with `keras`\n",
    "\n",
    "In this task you will train your own keras deep learning model with SGD as optimizer with the help of callbacks.\n",
    "\n",
    "### Instructions\n",
    "- Compile the model with `'sgd'` as optimizer, `'categorical_cross_entropy'` as the loss function and `['accuracy']` as the metric\n",
    "- Define a callback `early_stop = EarlyStopping(patience=5)`\n",
    "- Then fit the model on training data with `validation_split=0.1` and 20 epochs. This time the validation data is coming straight from the training data itself and also specify callbacks. Save it to a variable `history`\n",
    "- Now plot the validation curves using the code snippet from the theory above\n",
    "- Evaluate your model on test data using `.evaluate()` method of `model` on `X_test` abd `y_test`. It returns a tuple of loss and accuracy. Save them as `test_loss` and `test_acc`. Print out both to see their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/20\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5070 - acc: 0.7924 - val_loss: 0.4623 - val_acc: 0.8037\n",
      "Epoch 2/20\n",
      "7200/7200 [==============================] - 0s 52us/step - loss: 0.4465 - acc: 0.7988 - val_loss: 0.4305 - val_acc: 0.8113\n",
      "Epoch 3/20\n",
      "7200/7200 [==============================] - 0s 52us/step - loss: 0.4238 - acc: 0.8165 - val_loss: 0.4174 - val_acc: 0.8363\n",
      "Epoch 4/20\n",
      "7200/7200 [==============================] - 0s 58us/step - loss: 0.4116 - acc: 0.8289 - val_loss: 0.4069 - val_acc: 0.8400\n",
      "Epoch 5/20\n",
      "7200/7200 [==============================] - 0s 58us/step - loss: 0.4017 - acc: 0.8343 - val_loss: 0.3983 - val_acc: 0.8350\n",
      "Epoch 6/20\n",
      "7200/7200 [==============================] - 0s 52us/step - loss: 0.3932 - acc: 0.8400 - val_loss: 0.3921 - val_acc: 0.8350\n",
      "Epoch 7/20\n",
      "7200/7200 [==============================] - 1s 76us/step - loss: 0.3852 - acc: 0.8436 - val_loss: 0.3840 - val_acc: 0.8438\n",
      "Epoch 8/20\n",
      "7200/7200 [==============================] - 0s 54us/step - loss: 0.3776 - acc: 0.8481 - val_loss: 0.3778 - val_acc: 0.8425\n",
      "Epoch 9/20\n",
      "7200/7200 [==============================] - 0s 56us/step - loss: 0.3712 - acc: 0.8506 - val_loss: 0.3719 - val_acc: 0.8475\n",
      "Epoch 10/20\n",
      "7200/7200 [==============================] - 0s 58us/step - loss: 0.3658 - acc: 0.8517 - val_loss: 0.3680 - val_acc: 0.8425\n",
      "Epoch 11/20\n",
      "7200/7200 [==============================] - 0s 60us/step - loss: 0.3612 - acc: 0.8529 - val_loss: 0.3655 - val_acc: 0.8488\n",
      "Epoch 12/20\n",
      "7200/7200 [==============================] - 0s 51us/step - loss: 0.3578 - acc: 0.8540 - val_loss: 0.3654 - val_acc: 0.8488\n",
      "Epoch 13/20\n",
      "7200/7200 [==============================] - 0s 50us/step - loss: 0.3554 - acc: 0.8562 - val_loss: 0.3610 - val_acc: 0.8500\n",
      "Epoch 14/20\n",
      "7200/7200 [==============================] - 0s 64us/step - loss: 0.3530 - acc: 0.8569 - val_loss: 0.3581 - val_acc: 0.8525\n",
      "Epoch 15/20\n",
      "7200/7200 [==============================] - 0s 55us/step - loss: 0.3509 - acc: 0.8550 - val_loss: 0.3565 - val_acc: 0.8512\n",
      "Epoch 16/20\n",
      "7200/7200 [==============================] - 0s 50us/step - loss: 0.3492 - acc: 0.8576 - val_loss: 0.3551 - val_acc: 0.8525\n",
      "Epoch 17/20\n",
      "7200/7200 [==============================] - 0s 52us/step - loss: 0.3481 - acc: 0.8581 - val_loss: 0.3532 - val_acc: 0.8525\n",
      "Epoch 18/20\n",
      "7200/7200 [==============================] - 1s 124us/step - loss: 0.3464 - acc: 0.8572 - val_loss: 0.3522 - val_acc: 0.8550\n",
      "Epoch 19/20\n",
      "7200/7200 [==============================] - 1s 128us/step - loss: 0.3457 - acc: 0.8589 - val_loss: 0.3524 - val_acc: 0.8500\n",
      "Epoch 20/20\n",
      "7200/7200 [==============================] - 1s 110us/step - loss: 0.3443 - acc: 0.8603 - val_loss: 0.3519 - val_acc: 0.8500\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 32us/step\n",
      "test_acc:  0.86\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Code starts here\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# callbacks\n",
    "early_stop = EarlyStopping(patience=5)\n",
    "\n",
    "# fit model\n",
    "history = model.fit(X_train, y_train, validation_split=0.1, epochs=20, callbacks=[early_stop])\n",
    "\n",
    "# plot validation curves\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# evaluate on test data\n",
    "test_loss, test_acc = model.evaluate(X_test,y_test)\n",
    "print('test_acc: ',test_acc)\n",
    "\n",
    "# Code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ann_visualizer.visualize import ann_viz\n",
    "ann_viz(model, title=\"Artificial Neural network - Model Visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Parameters and hyperparameters in deep networks\n",
    "\n",
    "***\n",
    "\n",
    "__Parameters__ : \n",
    "- A parameter is a configuration variable that is internal to the model and whose value can be estimated from data\n",
    "- They are required by the model when making predictions \n",
    "- They are estimated or learned from data\n",
    "- They are often not set manually\n",
    "- Eg. Edge weights and bias terms are parameters\n",
    "\n",
    "You have already learnt how to find the best set of parameters by fitting a model.\n",
    "\n",
    "__Hyperparameters__ : \n",
    "- A hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data\n",
    "- They are used in processes to help estimate model parameters\n",
    "- They are set manually and often using heuristics\n",
    "- Eg. Learning rate, model architecture etc.\n",
    "\n",
    "The question now is how to find the best set of hyperparameters? Like the machine learning paradigm there are two approaches: **grid search** and **random search**\n",
    "\n",
    "**Grid search**: It systematically searches for optimal values of hyperparameter tuning from points on a grid.  \n",
    "Eg. If we have two hyperparameters $\\alpha$ and $\\beta$, then grid search may evaluate $(\\alpha,\\beta) = \\left\\{ (0.01,0.01),(0.01,0.03),(0.03,0.01), (0.03,0.03) \\right\\}$. \n",
    "\n",
    "A sample code snippet for carrying out such a method is given below:\n",
    "\n",
    "```python\n",
    "# import packages\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# try-out these learning rates\n",
    "lr_arr = np.arange(0.005,0.01,0.001)\n",
    "\n",
    "# grid search on learning rate\n",
    "for lr in lr_arr:\n",
    "    sgd = SGD(lr)\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "    results = model.evaluate(X_test, y_test,verbose=0)\n",
    "    print('For Learning Rate =', lr, \", Loss = \", results[0], \"and Accuracy =\", results[1])\n",
    "    print('='*50)\n",
    "```\n",
    "which outputs this kind of result:\n",
    "```python\n",
    "For Learning Rate = 0.005 , Loss =  1.1681998229026795 and Accuracy = 0.62\n",
    "==================================================\n",
    "For Learning Rate = 0.006 , Loss =  1.1341326189041139 and Accuracy = 0.605\n",
    "==================================================\n",
    "For Learning Rate = 0.007 , Loss =  1.0595148086547852 and Accuracy = 0.63\n",
    "==================================================\n",
    "For Learning Rate = 0.008 , Loss =  1.0890783977508545 and Accuracy = 0.65\n",
    "==================================================\n",
    "For Learning Rate = 0.009000000000000001 , Loss =  1.0610939478874206 and Accuracy = 0.635\n",
    "==================================================\n",
    "```\n",
    "\n",
    "**Random search**: As the name suggests it searches on randomly generated values of hyperparameters. Eg: If we have two hyperparameters $\\alpha$ and $\\beta$, then grid search may evaluate $(\\alpha,\\beta) = \\left\\{ (0.5519,0.7013),(0.9806,0.4251),(0.8064,0.6460), (0.0346,0.1419) \\right\\}$\n",
    "\n",
    "Random search if often a better method because if we find that the metrics do not depend on some hyperparameter after doing grid search then we will be left with much fewer values to find optimal value of the other hyperparameters.  \n",
    "\n",
    "Eg. In the above examples, if we find that the performance does not depend on the values of $\\beta$, then in the grid search case, we will be left with the performance measure only two values of $\\alpha$ while in the random search case we will be left with $4$ values. Below given is a sample code snippet for carrying out random search. \n",
    "\n",
    "```python\n",
    "# import packages\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# set of learning rates\n",
    "lr_arr = np.random.rand(10)\n",
    "\n",
    "# random search on learning rate\n",
    "for lr in lr_arr:\n",
    "    sgd = SGD(lr)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X, Y, epochs=10, batch_size=32, validation_data=(X_val, Y_val),verbose=0)\n",
    "    results = model.evaluate(X_val, Y_val,verbose=0)\n",
    "    print('For Learning Rate =', lr, \", Loss = \", results[0], \"and Accuracy =\", results[1])\n",
    "```\n",
    "\n",
    "**Note that since deep learning is a slow process, it might actually take lot of time to perform such type of optimization. Only if you have the compute resources you are encouraged to try this**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Exploding and vanishing gradients\n",
    "\n",
    "***\n",
    "\n",
    "To build a simple neural network is itself a chaotic task, moreover to add on to its chaotic behaviour we have to tune it to get better results. Hence there is need to throw light upon how to consider our input parameters i.e. weights and biases while building a neural network. \n",
    "\n",
    "First of all let's get revisit the notations and basics of building an end to end neural network.\n",
    "\n",
    "\n",
    "**Basics and Notations**\n",
    "\n",
    "Consider an `l` layer neural network, which has `l-1` hidden layers and 1 output layer. The parameters (weights and biases) of the layer `l` are represented as\n",
    "\n",
    "- $w^{[l]}$ - weight matrix of dimension(size of layer `l`, size of layer `l-1`)\n",
    "- $b^{[l]}$ - bias vectors of dimension(size of layer `l`, 1)\n",
    "\n",
    "In addition to weights and biases, during the training process, following intermediate variables are computed\n",
    "\n",
    "- $Z^{[l]}$ - Linear activations at layer `l`\n",
    "- $g^{[l]}(.)$ - Non-linear function\n",
    "- $A^{[l]}$ - Non-linear activations; output of $g^{[l]}(Z^{[l]})$, where $A^{[0]}$ is the input data X\n",
    "\n",
    "$$Z^{[l]} = w^{[l]} * A^{[l-1]} + b^{[l]}$$\n",
    "$$A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "As explained earlier, Training a neural network consists of 4 steps:\n",
    "\n",
    "- Initialize weights and biases.\n",
    "\n",
    "- **Forward propagation**: Using the input $X$, weights $w$ and biases $b$, for every layer we compute $Z$ and $A$. At the final layer, we compute $f(A^{[L-1]})$ which could be a sigmoid, softmax or linear function of $A^{[L-1]}$ and this gives the prediction $\\hat{y}$.\n",
    "\n",
    "- **Computing the loss function**:This step comprises of a function of the actual label $y$ and predicted label $\\hat{y}$. It captures how far off our predictions are from the actual target. Our objective is to minimize this loss function.\n",
    "\n",
    "- **Backward Propagation**: It involves calculating the gradients of the loss function $f(y,\\hat{y})$ with respect to $A$, $w$, and $b$ called $dA$, $dw$ and $db$. Using these gradients we update the values of the parameters from the last layer to the first. Repeat steps 2–4 for n iterations/epochs till we feel we have minimized the loss function, without overfitting the train data (more on this later!)\n",
    "\n",
    "Here’s a quick look at steps 2 , 3 and 4 for a network with 2 layers, i.e. one hidden layer. (Note that we haven’t added the bias terms here for simplicity):\n",
    "\n",
    "<img src='../images/w3.png'>\n",
    "\n",
    "So the first step towards building a neural network is initialization of parameters, if it is done correctly it could lead us to achieve optimization in least amount of time. \n",
    "\n",
    "> So now the question is: **Will initialization of weights have any effect on the learning process? If yes, then why and how? If no, then why?** Lets answer these questions\n",
    "\n",
    "Consider two scenarios of weight initialization:\n",
    "\n",
    "- **Zero intialization** : It refers to the process of setting all weights to $0$. In that case the derivative with respect to loss function is the same for every $w$ in $w^{[l]}$, thus, all the weights have the same values in the subsequent iteration. This makes the hidden units symmetric and continues for all the n iterations you run. **Setting weights as zero will reduce the term $\\mathbf{w^{[l]} * A^{[l-1]}}$ as zero which is similar to converting your neural network model to a linear model**. *It is important to note that setting biases to 0 will not create any troubles as non zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron are still different*.\n",
    "\n",
    "\n",
    "- **Random intialization**: This sort of scenario includes intializing random values to weights. Seems like this approach is better than previous as it does not assign zero weightage to weights, but wait there's a catch over here. While assigning the random values to weights we should take into the consideration its effects over activation function as it can attain high and low values which in turn may effect gradients. More often than not you will come across two major issues with random weight initialization strategy: **vanishing gradient** and **exploding gradient**. Lets discuss them in detail.\n",
    "\n",
    " \n",
    "**Vanishing and Exploding gradients**\n",
    "\n",
    "Neural networks learn through using iterative optimization algorithms such as gradient descent: they slowly make their way to the to find a local or global optima by updating weights in the direction favouring decrement of total cost value. Let's take a look at it mathematically\n",
    "\n",
    "\n",
    "$$\\text{repeat until} \\frac{\\partial J}{\\partial w ^{[l]} }\\to 0 :$$\n",
    "\n",
    "$$ w^{[l]} := w^{[l]}-\\alpha\\frac{\\partial J}{\\partial w ^{[l]} }$$\n",
    "\n",
    "\n",
    "Though we can actually set a hyper-parameter for max no. of iterations for gradient descent, but \n",
    "- If the no. of iterations are too small we can end up having inaccurate results for certain deep neural networks. In other words, convergence has not yet occured.\n",
    "- If iterations are too large the training period increases drastically. **This occurs mainly due to magnitude of change in gradient**. ***If gradient at each step is too small, then greater no. of iterations are required for convergence***; in other words the weights will not move as close to the minimum in the set number of iterations which becomes a problem to train the model, and they start predicting poorly.\n",
    "\n",
    "You can visualize this by having a look at the diagrams below:-\n",
    "\n",
    "\n",
    "<img src='../images/elongated_GD.png'>\n",
    "\n",
    "\n",
    "gradient descent having small updates in the weights vs compared to optimal gradient descent \n",
    "\n",
    "\n",
    "<img src='../images/optimal_GD.png'>\n",
    "\n",
    "\n",
    "Let's get a deep dive and actually understand how random intialization of weight is having an effect on model as whole.\n",
    "Recall the sigmoid function, as it squeezes any input value into an output range of $(0,1)$ ,hence it is perfectly suitable for classification and representation of probabilites which is why this function is used as an activation function in many deep neural network frameworks.\n",
    "\n",
    "$$\\text{Sigmoid} = S(\\alpha) = \\frac{\\mathrm{1} }{\\mathrm{1} + e^{- \\alpha }} $$\n",
    "\n",
    "Considering sigmoid as the activation function let's look at the derivative of it.\n",
    "\n",
    "$$\\frac{\\mathrm{1} }{\\mathrm{1} + e^{- \\alpha }}\\bigg[1-\\frac{\\mathrm{1}}{\\mathrm{1} + e^{-\\alpha}}\\bigg]$$\n",
    "\n",
    "which essentially can be reformed as $S(1-S)$. If we calculate  the maximum value this derivative function can achieve it comes out to be 1/4. In other words, the output of the derivative of the cost function is always between 0 and 1/4.\n",
    "\n",
    "\n",
    "For the simplicity sake we will be considering general structure of a simple, univariate neural network.\n",
    "<img src='../images/general_structure_nn.jpeg'>\n",
    "\n",
    "During the phase of backpropgation weights are modified through gradient descent such that output J is minimized. To calculate the derivative to the first weight, we used the chain rule to “backpropagate” like so:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w^{1}} = \\frac{\\partial J}{\\partial output}*\\frac{\\partial output}{\\partial hidden2}*\\frac{\\partial hidden2}{\\partial hidden1}*\\frac{\\partial hidden1}{\\partial w^{1}}$$\n",
    "\n",
    "\n",
    "As we know since the output is the activation of the 2nd hidden unit, and we are using the sigmoid function as our activation function, then the derivative of the output is going to contain the derivative of the sigmoid function. In specific, the resulting expression will be:\n",
    "\n",
    "From output to hidden2\n",
    "\n",
    "$$Z^{3} = hidden2*w^{3}$$\n",
    "\n",
    "$$\\frac{\\partial output}{\\partial hidden2} = \\frac{\\partial Sigmoid(Z^{3})}{\\partial Z^{3}}w^{3}$$\n",
    "\n",
    "From hidden2 to hidden1\n",
    "\n",
    "$$Z^{2} = hidden1*w^{2}$$\n",
    "\n",
    "$$\\frac{\\partial hidden2}{\\partial hidden1} = \\frac{\\partial Sigmoid(Z^{2})}{\\partial Z^{2}}w^{2}$$\n",
    "\n",
    "\n",
    "if we look at the combined equation of these two derivatives together \n",
    "\n",
    "$$\\frac{\\partial output}{\\partial hidden2}*\\frac{\\partial hidden2}{\\partial hidden1} = \\frac{\\partial{Sigmoid(Z^3)}}{\\partial Z^{3}}w^{3}*\\frac{\\partial{Sigmoid(Z^2)}}{\\partial \n",
    "Z^{2}}w^{2}$$\n",
    "\n",
    "\n",
    "\n",
    "Now let's take an example if the weights are assigned using standard approach which involves choosing the weights using a gaussian with mean 0 and standard deviation as 1. Hence, weight value assignment will be satisfying $\\mid{w^l}\\mid<1$ randomly.Also, derivative of sigmoid function can attain a value of maximum 1/4 or less than that.\n",
    "Hence, above equation will almost approximate to:\n",
    "\n",
    "$$\\frac{\\partial output}{\\partial hidden2}*\\frac{\\partial hidden2}{\\partial hidden1}\\approx (<=1/4)*(<1)*(<=1/4)*(<1)$$\n",
    "\n",
    "In above equation we are multiplying four values which are between 0 and 1. Hence, will have a resultant value which is very less. This neural network isn't that deep. Imagine the resulting magnitude in case of neural network having more than two layers. As we backpropagate further we would end up having even tinier gradient. The earlier layers are the slowest to train in such a case.The weight update is minor and results in slower convergence. This makes the optimization of the loss function slow. In the worst case, this may completely stop the neural network from training further.This phenomenon is called **vanishing gradients**.\n",
    "\n",
    "Let's take another example. This example is somewhat contrived but it has the virtue of firmly establishing the concept which is exactly opposite to vanishing gradients phenomenon. Assuming $w^1= w^2 = w^3 = 100$. Second we will choose biases with an equal negative value such that for individual activation functions at each layer the value of $Z^{l} = 0$\n",
    "which makes the derivative of $\\frac{\\partial Sigmoid(Z^j)}{\\partial Z^{j}} = 1/4$ and hence, the above equation reduces to \n",
    "\n",
    "$$\\frac{\\partial output}{\\partial hidden2}*\\frac{\\partial hidden2}{\\partial hidden1}\\approx (<=1/4)*(100)*(<=1/4)*(100)$$\n",
    "\n",
    "If we generalize above equation for a deep neural network having more than two layers. When these weights are multiplied along the layers, they cause a large change in the cost. Thus, the gradients are also going to be large. This means that the changes in w, by $w — \\alpha*\\partial w$, will be in huge steps, the downward moment will increase. This may result in oscillating around the minima or even overshooting the optimum again and again and the model will never learn! This phenomenon is called as **Exploding Gradients**.\n",
    "\n",
    "Another impact of exploding gradients is that huge values of the gradients may cause number overflow resulting in incorrect computations or introductions of NaN’s. This might also lead to the loss taking the value NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Tackling the problem of vanishing and exploding gradients\n",
    "\n",
    "***\n",
    "\n",
    "In the previous topic you learnt about vanishing and exploding gradients and saw how they can prevent the deep learning model from updating its parameters. Now lets explore some ideas to prevent this from happening.\n",
    "\n",
    "- **`RELU/ Leaky RELU as activation functions`**: In the chapter on activation functions you must have come across **ReLU** function.  It is relatively robust to the vanishing/exploding gradient issue (especially for networks that are not too deep) because of the constant gradient of $1$ over positive inputs. But its gradient is $0$ for negative inputs which might lead to something called the **Dying ReLU problem**. To counter this **Leaky ReLU** is a solution which never has $0$ gradient. Thus they never die and training continues. An image is shown below for Leaky RelU.\n",
    "\n",
    "<img src='../images/leaky_relu.png'>\n",
    "\n",
    "- **`Applying Heuristics`**: For deep networks, you can use a heuristic to initialize the weights depending on the non-linear activation function. Here, instead of drawing from standard normal distribution, we are drawing weight $w$ from normal distribution with variance $\\frac{k}{n}$, where $k$ depends on the activation function. While these heuristics do not completely solve the exploding/vanishing gradients issue, they help mitigate it to a great extent. The most common are:\n",
    "    - **For RELU(z)**: We multiply the randomly generated values of $w$ by: $\\sqrt {\\frac {2}{size^{[l-1]}}}$\n",
    "\n",
    "        $w^{[l]}$ = `np.random.randn(size_l, size_l-1)*np.sqrt(2/size_l-1)`\n",
    "\n",
    " - **For tanh(z)**: The heuristic is called Xavier initialization. It is similar to the previous one, except that $k$ is $1$ instead of $2$.\n",
    "i.e. $$\\sqrt {\\frac {1}{size^{[l-1]}}}$$\n",
    "$w^{[l]}$ = `np.random.randn(size_l, size_l-1)*np.sqrt(2/size_l-1)`\n",
    "\n",
    "Go through the [official documentation](https://keras.io/initializers/) of weight initialization with keras to explore more about weight initialization strategies.\n",
    "\n",
    "In Keras you can initialize weights from a random uniform distribution and bias as zeros in the following manner:\n",
    "```python\n",
    "model.add(Dense(64,kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros'))\n",
    "```\n",
    "\n",
    "   - **Another commonly used heuristic**: <img src='../images/w6.png'> These serve as good starting points for initialization and mitigate the chances of exploding or vanishing gradients. They set the weights neither too much bigger that $1$, nor too much less than $1$. So, the gradients do not vanish or explode too quickly. They help avoid slow convergence, also ensuring that we do not keep oscillating off the minima. There exist other variants of the above, where the main objective again is to minimize the variance of the parameters.\n",
    "\n",
    "\n",
    "- **`Gradient Clipping`** This is another way of dealing with the exploding gradient problem. We set a threshold value, and if a chosen function of a gradient is larger than this threshold, we set it to another value. An important point to note is that we have talked about various initializations of $w$, but not the biases $b$. This is because the gradients with respect to bias depend only on the linear activation of that layer, and not on the gradients of the deeper layers. Thus there is no diminishing or explosion of gradients for the bias terms. As mentioned earlier, they can be safely initialized to $0$. An example of gradient clipping is shown below: \n",
    "\n",
    "$$x_{clipped} = \n",
    "\\begin{cases} \n",
    "x_{max} \\qquad\\text{ if } x > x_{max}\\\\\n",
    "x \\qquad \\text{ if } x_{min} \\le x \\le x_{max}\\\\\n",
    "x_{min} \\qquad\\text{ if } x \\lt x_{min}\n",
    "\\end{cases}$$\n",
    "<br>\n",
    "<center> \n",
    "</center><br>\n",
    "\n",
    "```python\n",
    "x_clipped = np.min(np.max(x,x_min),x_max)\n",
    "```\n",
    "\n",
    "- **`Regularization`**: Regularization prevents weights from becoming too large by imposing a cost on large weights. Due to this, the activations and the gradients can't become too large preventing the problem of exploding gradients. It is exactly the same regularization concept that you have come across in machine learning paradigm. Go through the [documentation on regularizers](https://keras.io/regularizers/) with keras to have an extensive look. You can have separate regularizers for weights and bias for ex in the below code snippet:\n",
    "```python\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "               bias_regularizer=regularizers.l1(0.01)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz\n",
    "\n",
    "1. Which of the following is an issue with ReLU activation?\n",
    "\n",
    "    (a) Vanishing gradient\n",
    "    \n",
    "    (b) Dying gradient\n",
    "    \n",
    "**ANS**: (b) Dying gradient\n",
    "\n",
    "**Explaination**: For negative inputs, the output is 0 with ReLU activation. This doesn't lead to weight updates and is called the dying gradient problem\n",
    "\n",
    "\n",
    "2. Is setting weights to zeros a good idea in deep learning?\n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (a) YES\n",
    "\n",
    "**Explaination**: Setting weights to zero will hinder the network from learning anything as the incoming input to any neuron will be zero/bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept-end quiz\n",
    "\n",
    "1. ReLU can blow up activations. \n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (a) YES\n",
    "\n",
    "**Explaination**: for positive inputs, ReLU has no upper limit. Hence it can blow up activations\n",
    "\n",
    "\n",
    "2. Will setting bias as zeros have the same effect as setting weights to zero during initialization?\n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (b) NO\n",
    "\n",
    "**Explaination**: Setting bias to zeros will not prevent the network from updating both bias and weights. But setting weights to zeros will not update the weights at all.\n",
    "\n",
    "\n",
    "3. What advantage does ReLU hold over sigmoid function?\n",
    "\n",
    "    (a) Solves the issue of vanishing gradients\n",
    "    \n",
    "    (b) Easier to compute\n",
    "    \n",
    "    (c) Both (a) & (b)\n",
    "    \n",
    "    (d) None of the above\n",
    "    \n",
    "**ANS**: (c) Both (a) & (b)\n",
    "\n",
    "**Explaination**: ReLU solves the issue of vanishing gradients as well as is far easier to compute than sigmoid which involves calculating the exponentials\n",
    "\n",
    "\n",
    "4. Which of the following methods can be used to address the issue of exploding gradients?\n",
    "\n",
    "    (a) Gradient clipping\n",
    "    \n",
    "    (b) Regularization\n",
    "    \n",
    "    (c) Both (a) & (b)\n",
    "    \n",
    "    (d) None of the above\n",
    "    \n",
    "**ANS**: (c) Both (a) and (b)\n",
    "\n",
    "**Explaination**: Both gradient clipping and regularization can be used to solve the problem of exploding gradients.\n",
    "    \n",
    "    \n",
    "5. TanH is a scaled version of sigmoid.\n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (a) YES\n",
    "\n",
    "**Explaination**: TanH can ber written in terms of sigmoid as: tanh(𝑥)=2𝜎(2𝑥)−1\n",
    "\n",
    "\n",
    "6. Vanishing gradients do not occur with tanH activation.\n",
    "\n",
    "    (a) TRUE\n",
    "    \n",
    "    (b) FALSE\n",
    "    \n",
    "**ANS**: (b) FALSE\n",
    "\n",
    "**Explaination**: Vanishing gradients do occur with tanH activation as its range lies between -1 and 1\n",
    "\n",
    "\n",
    "7. Suppose you are solving a problem where you have to classify images into 3 categories. Which evaluation metric would you choose?\n",
    "\n",
    "    (a) Cross entropy\n",
    "    \n",
    "    (b) Softmax\n",
    "    \n",
    "    (c) Accuracy\n",
    "    \n",
    "    (d) None of these\n",
    "    \n",
    "    \n",
    "**ANS**: (c) Accuracy\n",
    "\n",
    "**Explainatin**: Cross entropy is a loss function, softmax is used to normalize inputs to 1. Only accuracy can be used as evaluation metric\n",
    "\n",
    "\n",
    "8. ReLU can produce sparser outputs as compared to other activations.\n",
    "\n",
    "    (a) YES\n",
    "    \n",
    "    (b) NO\n",
    "    \n",
    "**ANS**: (a) YES\n",
    "\n",
    "**Explaination**: ReLU produces a zero valued output for negative inputs. This leads to sparse outputs.\n",
    "\n",
    "\n",
    "9. Deeper the network (suppose you have tanH and sigmoid activations), more is the chance of vanishing gradients. \n",
    "\n",
    "    (a) TRUE\n",
    "    \n",
    "    (b) FALSE\n",
    "    \n",
    "**ANS**: (a) TRUE\n",
    "\n",
    "**Explaination**: Deeper the network, more the number of terms in calculating the gradient. This has more terms in the range $0-\\frac{1}{4}$ and has higher chances of vanishing gradients than shallow networks.\n",
    "\n",
    "\n",
    "10. Is the number of hidden layers of a neural network a parameter or hyperparameter?\n",
    "\n",
    "    (a) Parameter\n",
    "    \n",
    "    (b) Hyperparameter\n",
    "    \n",
    "**ANS**: (b) Hyperparameter\n",
    "\n",
    "**Explaination**: The number of hidden layers of a neural network are pre-specified and is not learnt during the learning process i.e. backpropagation. Therefore, it is a hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
