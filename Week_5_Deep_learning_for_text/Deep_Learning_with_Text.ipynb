{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning For Text\n",
    "\n",
    "## Description\n",
    "\n",
    "Processing language is difficult owing to the sequential nature of it.\n",
    "Learn and understand how to utilize the power of Deep Learning(RNNs, LSTM, GRU) for NLP tasks.\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "- Need of sequence modelling\n",
    "- Recurrent neural networks\n",
    "- LSTM and its variants\n",
    "\n",
    "## Pre-requisite\n",
    "\n",
    "- Python (along with NumPy and pandas libraries)\n",
    "- Word Embeddings\n",
    "- Basic statistics \n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "\n",
    "- Understand the problem of handling sequence models\n",
    "- Learn about RNNs and how to implement it\n",
    "- Learn about LSTMs and its variants and how to implement it\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Why sequence models?\n",
    "\n",
    "Repeatedly throughout our previous NLP concepts, we have mentioned why processing text by machines is a challenge. Textual data by it's nature requires the understanding of context which machines can't do easily.\n",
    "\n",
    "One of the recent(and popular) techniques to have machine understand the context is 'word embeddings'.\n",
    "\n",
    "They are based on distributional hypothesis — words appearing within similar context possess similar meaning. \n",
    " \n",
    "\n",
    "![](final_images/word_vector.jpg)\n",
    "\n",
    "Word embeddings are pre-trained on a task where the objective is to predict a word based on its context using neural networks. Using word embeddings Neural-based models have achieved superior results on various language-related tasks. The word vectors intrinsically embed syntactical and semantic information. As a result, they can efficiently construct high-quality word embeddings (For e.g. 'man' + 'woman'- 'king' = 'queen').\n",
    "\n",
    "\n",
    "The limitation with word embedding methods is when we want to obtain vector representations for sentences/phrases such as \"blue moon\" or “USA Today”. The difficutly is in combining the individual word vector representations since the phrases don’t represent the combination of meaning of the individual words. \n",
    "\n",
    "- 'Blue moon' refers to a phenomenon that happens very rarely(and not blue colored moon)\n",
    "\n",
    "- 'USA Today' refers to newspaper(and not a phrase talking about the country today) \n",
    "\n",
    "\n",
    "It gets more complicated when longer phrases and sentences are considered.\n",
    "\n",
    "Consider the following use cases:\n",
    "\n",
    "\n",
    "- **Sentiment Classification:** The input X is a sentence say \"There is nothing to love in this movie\" and output is how many stars the review is giving. \n",
    "\n",
    "\n",
    "![](final_images/sentiment_analysis.jpg)\n",
    "\n",
    "- **Machine Translation:** \n",
    "\n",
    "This is one of the popular NLP use cases given the rise of globalization.\n",
    "\n",
    "In this task, given words, phrase or sentence in one language, the machine automatically translates it into another language. We input a sentence in Russian and we want our output to be the same sentence, say in English.\n",
    "\n",
    "![](final_images/mt.jpg)\n",
    "\n",
    "\n",
    "- **Name Entity Recognition** The input is a sentence say \"Andrew Ng was a former Vice President and Chief Scientist at Baidu\" and the output is prediction of the named entities in that sentence.\n",
    "\n",
    "\n",
    "![](final_images/ner.jpg)\n",
    "\n",
    "There are bunch of areas in which NER Systems are used. Some of the use cases are:\n",
    "\n",
    "- In news, identify the main subjects in the news.\n",
    "\n",
    "- To find a relation between various entities described in a document\n",
    "\n",
    "****\n",
    "\n",
    "All the above uses cases are examples where one can't work if the data input is not considered to be a sequence.\n",
    "\n",
    "For e.g. \n",
    "\n",
    "#1.\n",
    "Consider the following review: \"I have nothing but love for the movie\"\n",
    "\n",
    "If the sequence info is not taken, it becomes a negative sentiment, but in sequence it is a positive sentiment. \n",
    "\n",
    "#2.\n",
    "Consider the English sentence \"Hello world\"(2 words). It's French translation is \"Bonjour le monde\"(3 words). In language translation, the input sentence and output sentence are usually not of the same length. If not considered sequence how can we decide the input length or output length?!\n",
    "\n",
    "\n",
    "We don't realise it but humans do sequence processing intrinsically. Humans don’t isolate events and start their thinking from scratch every second. We are able to 'watch a movie' or 'read a book' precisely because we make a mental note about all the things that happened in the movie/book and how they affect the current/future events.  \n",
    "\n",
    "Traditional neural networks are unable to do this and is one of its major shortcomings. \n",
    "\n",
    "For example, imagine you want to classify what kind of event is happening at every page in a book. A traditional neural network can't use its reasoning about previous events in the book to inform later ones.\n",
    "\n",
    "Another major limitation of traditional neural networks is that their parameters are too constrained. They accept a fixed-sized vector as input and produce a fixed-sized vector as output. When dealing with say, `machine translation` how do you decide the input length or output length?(Remember the French-English translation problem?)\n",
    "\n",
    "\n",
    "Enter `'Recurrent Neural Networks'`\n",
    "\n",
    "Recurrent neural networks are a variation of neural network designed to recognize patterns in sequences of data. These algorithms take both `time` and `sequence`, they have a temporal dimension.\n",
    "\n",
    "Let's look at it in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Recurrent Neural Network Model\n",
    "\n",
    "\n",
    "**Intuition behind RNN**\n",
    "\n",
    "Let's break down how humans process sequences to understand what's needed for machines to do the same.\n",
    "\n",
    "Suppose you are watching a Tom Cruise action movie.\n",
    "\n",
    "`In one scene, Tom Cruise drives from his home and arrives at a supermarket. In the next scene, it's shown that he is holding an Orange fruit. In the next scene, a man tries to attack Tom but he saves himself by spraying Orange juice on the attacker's face.` \n",
    "\n",
    "\n",
    "Here's how you understood the movie:\n",
    "\n",
    "- From the first scene, you identified he's in supermarket\n",
    "\n",
    "- In the next scene, without any distinctive supermarket features, when you saw the image of Tom with oranges you categorized it as shopping instead of cooking/eating.\n",
    "\n",
    "- In the final scene, when Tom attacked the attacker with oranges, you had no problem following that scene because you remembered that Tom was holding an orange\n",
    "\n",
    "\n",
    "Following is what a model will need to do the same:\n",
    "\n",
    "\n",
    "- After seeing each scene, the model should output a label as well as update the knowledge it’s been learning.\n",
    "\n",
    "For the movie, when seeing the first scene,the model has to output that Tom is in a supermarket and keep this knowledge with him, while additionaly scraping the knowledge of Tom driving from home.\n",
    "\n",
    "Similarly, the model might learn to automatically discover and track information like time of day(if a scene contains an image of the setting sun, the model should remember that it's evening), and within-movie progress (is this image the first frame or the 10th?).\n",
    "\n",
    "Most critical though your model should automatically discover useful information by itself(just as a neural network is able to learn good features with the help of data). \n",
    "\n",
    "\n",
    "- When given a new scene, the model should incorporate the knowledge it has learned so far to do a better job.\n",
    "\n",
    "For the movie, when seeing the second scene, the model should categorize the activity as shopping(as it has learned that Tom was in a supermarket) instead of eating.\n",
    "\n",
    "\n",
    "Recurrent Neural Networks(RNNs) are a family of neural networks for processing sequential data.\n",
    "\n",
    "**Recurrent Neural Networks(RNNs)**\n",
    "\n",
    "Recurrent neural networks (RNNs) are designed to utilize and learn from sequential information. The RNN architecture is supposed to perform the same task for every element of a sequence, and hence the term `recurrent` in its nomenclature. RNNs have been of great use in the task of natural language processing because of the sequential dependency of words in any language.  \n",
    "\n",
    "\n",
    "RNNs are specialized for processing a sequence of values $x_1, . . . , x_t$ and can scale to much longer sequences than would be feasible for networks without sequence-based specialization. Most RNNs can also process sequences of varible length.\n",
    "\n",
    "In a RNN, the information cycles through a loop. When it makes a decision, it takes into consideration the current input and also what it has learned from the inputs it received previously.\n",
    "\n",
    "\n",
    "\n",
    "The two images below illustrate the difference in the information flow between a traditional Neural Network & RNNs.\n",
    "\n",
    "\n",
    "![](final_images/rvsf.jpg)\n",
    "\n",
    "\n",
    "As you can see from the above image, the difference is that RNNs have loops.\n",
    "\n",
    "![](final_images/new_rnn_1.jpg)\n",
    "\n",
    "In the above diagram, the Neural Network looks at the input xt and outputs a value ht. The loop allows information to be passed from one step to the next.\n",
    "\n",
    "Don't be baffled by the loop. Another way to think about RNN is to think of it as multiple copies of the same network, each passing the message to its successor. \n",
    "\n",
    "Following is the same network as above with it's loop unfurled:\n",
    "\n",
    "![](final_images/new_rnn_2.jpg)\n",
    "\n",
    "This chain-like structure can be visualised as being a sequence itself and RNN therefore is the optimum architecture of neural network to use for such data.\n",
    "\n",
    "Generally, at any time step of a sequence, RNNs compute some memory based on its computations thus far; i.e., prior memory and the current input. This computed memory is used to make predictions for the current time step and is passed on to the next step as an input. An RNN `recursively` applies a computation to every instance of the input sequence therefore conditioning itself on the previous computed results as well. An RNN therefore take as their input not just the current input example they see, but also what they have perceived previously in time. \n",
    "\n",
    "So recurrent networks have two inputs, the present as well as the recent past, which together determines how they respond to the new data( just like we do in real life). Recurrent networks therefore are sometimes said to have `memory`. \n",
    "\n",
    "\n",
    "This addition of memory to neural networks has a major purpose: There is information in the sequence itself, and recurrent nets use it to perform tasks that traditional neural networks can't.\n",
    "\n",
    "It is essentially finding correlations between events separated by time, one way to think about RNNs therfore is they are a way to share weights over time. \n",
    "\n",
    "*Note: These correlations are called “long-term dependencies”*\n",
    "\n",
    "One would think that the current output depends directly on the previous output. That's not the case. One important thing to understand is relationship between present input and past input(`memory`) is indirect. It’s indirect because the current output is dependent on the previously calculated `hidden states`, not on the previous outputs.\n",
    "\n",
    "For example, given the sentence \"I like eating noodles\", the RNN doesn’t deduce \"noodles\" directly from \"eating\", it deduces \"noodles\", partly, from the information(the hidden state) that gave rise to \"eating\".\n",
    "\n",
    "Think of it as human memory that circulates invisibly within our brain, affecting our behavior without never revealing the full shape of it. You know it's there, you use it but you never give it proper form. Information circulates in the hidden states of recurrent nets in a similar way.\n",
    "\n",
    "Sounds confusing? That's because it is. \n",
    "\n",
    "The exact relationship between present and past input depends on the RNN’s weights. Creating a coherent sequence as we go along is only possible if one can recall what came before. And RNNs do exactly that; they remember what came before. Obviously, RNNs are not magic models. They need to be trained. They work because trained networks identifies and learns patterns in data. \n",
    "\n",
    "***\n",
    "**Dive Deeper(Optional)**\n",
    "\n",
    "\n",
    "Let's now try to understand this process better by understandig it mathematically.\n",
    "\n",
    "\n",
    "Following is the (deceptively) simple equation of RNN to carry memory forward:\n",
    "\n",
    "$$h_t= \\phi(W_{x_t} + Uh_{t-1})$$\n",
    "\n",
    "The hidden state at time step t is h_t. It is a function of:\n",
    "\n",
    "- $x_t$(input of current time step t) modified by a weight matrix W \n",
    "- The hidden state of the previous time step $h_t-1$ multiplied by its own hidden-state-to-hidden-state matrix U(called transition matrix) \n",
    "\n",
    "\n",
    "The weight matrices(Both W and U) are the filters that determine how much significance to give to both the current input and the past hidden state(Note here we are talking about past hidden state and not past input).\n",
    "\n",
    "\n",
    "These weights are optimally learned using the backpropagation(similar to how weights are learned in traditional neural networks) \n",
    "Only here we are adding a time step and calling it `backpropogation through time(BPTT)`. Though there’s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! \n",
    "\n",
    "Remember that when you \"unroll\" or \"expand\" an RNN, it essentially becomes a feedforward network. There’s more work to do to compute the gradients; `time`, in this case, is expressed by well-defined & properly ordered series of calculations linking one time step to the next time step, but that backprop works almost the same way for recurrent nets that it would for traditional ones.\n",
    " \n",
    "The following diagram will help you visualise how it works:\n",
    "\n",
    "![](final_images/backprop.gif)\n",
    "\n",
    "In the diagram above, each x(input example) is multiplied with their current filter w,(weights) as well as with a processed(hidden) state of previous input state.\n",
    "\n",
    "If you are able to follow through the above image, congrats now you understand how RNN works(atleast at a theoretical level). For those who aren't able to, let's use an example.\n",
    "\n",
    "**End of Dive Deeper(Optional)**\n",
    "***\n",
    "\n",
    "*Example:*\n",
    "\n",
    "Suppose you had to predict the word 'data' using a RNN.\n",
    "\n",
    "The neural network has the vocabulary: d, a, t. Exactly enough to produce the word 'data'. \n",
    "\n",
    "We input the first character, \"d\" and from there expect the output at the following timesteps to be: \"a\", \"t\" and \"a\" respectively, to form:\n",
    "\n",
    "`DATA`\n",
    "\n",
    "In numerical form, it will look something like this:\n",
    "\n",
    "![](final_images/new_rnn_4.jpg)\n",
    "\n",
    "\n",
    "\n",
    "The RNN therfore is learning, given \"d\", \"a\" is most likely to be the next character. Given \"da\", \"t\" is the next likely character and with \"data\", the final character should be \"a\".\n",
    "\n",
    "But, if the neural network wasn’t trained on the word \"data\", and thus didn’t have optimal weights (ie. just randomly initialized weights), then we’d have garble like \"dtaa\" coming out.\n",
    "\n",
    "\n",
    "Now is a good time to point out the concept of `start` & `end` tokens.\n",
    "The reason why RNNs work so effectively is their ability to have any length input/output vectors.\n",
    "\n",
    "`<START>` and `<END>` tokens tell us when input begins and when output ends. \n",
    "\n",
    "In our previous example, when the final character \"a\" is outputted , `<END>` token is placed; this tells the RNN that it has completed the word processing. \n",
    "\n",
    "Its effectiveness is much more visible when we talk about other RNN application like 'Image Captioning'. In image captioning, depending on the image no. of words in the caption could be n words long.It is the end token which helps the model know that the caption has been completed(as oppposed to running a loop or fixing a max value for n)\n",
    "\n",
    "`<START>` and `<END>` tokens therfore help RNNs anticipate the processing required.\n",
    "\n",
    "One small thing though, RNNs don't automatically learn them. We ourselves have to add these tokens while training the data.\n",
    "\n",
    "Lot of theory, eh?\n",
    "\n",
    "Let's get our hands dirty with coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (8982, 500)\n",
      "input_test shape: (2246, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import reuters\n",
    "max_features = 10000  # number of words to consider as features\n",
    "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)\n",
    "\n",
    "#one hot encode labes\n",
    "one_hot_train_labels = to_categorical(y_train)\n",
    "one_hot_test_labels = to_categorical(y_test)\n",
    "\n",
    "#train test\n",
    "x_train = input_train\n",
    "\n",
    "y_train = one_hot_train_labels\n",
    "\n",
    "#train test\n",
    "x_test = input_test\n",
    "\n",
    "y_test = one_hot_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Data has already been loaded in x_train, y_train, x_test, y_test\n",
    "\n",
    "- Create a `\"Sequential()\"` model and save it in a variable called `model`\n",
    "\n",
    "- Add an embedding layer `\"Embedding(max_features,100)\"` to `model`. This is to take embedding the news data.\n",
    "\n",
    "- Add RNN model `\"SimpleRNN\"` with 32 layers to `model`. Note this is the RNN model you are building.\n",
    "\n",
    "- Next, create the output layer. For this, add `\"Dense(46, activation='softmax')\"` to `model`. 46 is the number of classes the output has.\n",
    "\n",
    "- Compile the model using `\"compile()\"` attribute of `model` and pass the following parameters in it: `optimizer='adam'`, `loss='categorical_crossentropy'`, `metrics=['acc']`\n",
    "\n",
    "- Fit the model using `\"fit()\"` attibute of `model` and pass the following parameters in it: `x_train`, `y_train`,`verbose=2`,`epochs=5`,`batch_size=128`,`validation_split=.2`\n",
    "\n",
    "- Evaluate the model using `\"evaluate()\"` attribute of `model`. Pass `x_train`,`y_train` as the parameters and store the result of it in a variable called `test_score`\n",
    "\n",
    "***Note:*** After submitting once(and successfully passing the test cases), feel free to play around with the model parameters\n",
    "\n",
    "## Test cases\n",
    "\n",
    "#test_score\n",
    "\n",
    "Variable declaration\n",
    "\n",
    "test_score[1]+0.01>=round(0.5084594835793452,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/5\n",
      " - 13s - loss: 3.0128 - acc: 0.2679 - val_loss: 2.4564 - val_acc: 0.3450\n",
      "Epoch 2/5\n",
      " - 12s - loss: 2.4208 - acc: 0.3479 - val_loss: 2.3478 - val_acc: 0.3450\n",
      "Epoch 3/5\n",
      " - 13s - loss: 2.2224 - acc: 0.4242 - val_loss: 2.2190 - val_acc: 0.4062\n",
      "Epoch 4/5\n",
      " - 13s - loss: 1.9052 - acc: 0.5219 - val_loss: 2.0144 - val_acc: 0.4880\n",
      "Epoch 5/5\n",
      " - 14s - loss: 1.5727 - acc: 0.6007 - val_loss: 2.0210 - val_acc: 0.4691\n",
      "2246/2246 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout, SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(x_train, y_train,verbose=2,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_split=.2)\n",
    "\n",
    "test_score=model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Problem with RNNs\n",
    "\n",
    "So far RNNs seems fantastic and State of the Art(SOTA) for text processing, right?\n",
    "\n",
    "Well, they are great except for one **major** problem. Let's see what it is\n",
    "\n",
    "Continuing with our Tom Cruise movie example, we have placed no constraints on updates, so the knowledge can change chaotically within RNN.\n",
    "\n",
    "In one frame it sees the characters eating noodles and might conclude they are in Korea, and at the next frame it sees penguins and thinks they are in antartic. Or perhaps it has lot of information to suggest that Tom is a lawyer but then suggests he is a professional assassin after seeing him in a karate class.\n",
    "\n",
    "This chaos means information transforms and vanishes quickly, and it's difficult for the model to keep a 'long-term memory'.\n",
    "There are cases where we only need to look at recent information to perform the present task. \n",
    "\n",
    "\n",
    "For example, consider a language model predicting the next word. If we are trying to predict the last word in “the sun rises in the east,” we don’t need much context – it’s obvious the word is going to be east. In such cases,the dependency gap between the relevant information is small, RNNs can learn to use such information.\n",
    "\n",
    "Unfortunately as the gap grows, RNNs inefficiency also grows. In theory, RNNs are absolutely capable of handling such \"long-term dependencies.\" In practice though, RNNs don’t seem to be able to cope with them. Let's try to understand why.\n",
    "\n",
    "# Gradient issue\n",
    "\n",
    "The basic problem in RNNs is that gradients propagated over many stages tend to either vanish (most of the time) or explode (rarely, but with much damage to the optimization).\n",
    "\n",
    "This is in part because the information that is flowing through neural nets passes through many stages of multiplication.\n",
    "\n",
    "Any quantity multiplied frequently by an amount slightly greater than one can become immeasurably large (For eg. Calculation of compound interest that ends up having the borrower pay much more than the original amount). At the same time, its inverse i.e. multiplying by a quantity less than one, is also true( For e.g Gamblers who win 99 cents on every dollar, go bankrupt fast)\n",
    "\n",
    "Because the layers and time steps of deep neural networks relate to each other through multiplication, derivatives are prone to vanishing or exploding.\n",
    "\n",
    "\n",
    "\n",
    "### Vanishing gradient\n",
    "\n",
    "![](final_images/vanishing_gradient.jpg)\n",
    "\n",
    "\n",
    "So in recurrent neural networks, layers that get a small gradient update stops learning. Those are usually the earlier layers. So because these layers don’t learn, RNN’s can forget what it seen in longer sequences, thus having a short-term memory.\n",
    "\n",
    "\n",
    "### Exploding gradient\n",
    "\n",
    "On the other hand, exploding gradients treat every weight as though it is extremely important. This means that the computation within the RNN can potentially blow up to infinity without sensible weights. This makes learning VERY unstable because a slight shift in the weights in the wrong direction during backprop can blow up the activations during the forward pass. \n",
    "\n",
    "But exploding gradients can be solved relatively easily, because they can be truncated or squashed using sigmoid/tanh(This often results in vanishing gradient problem though). \n",
    "\n",
    "![](final_images/exploding_gradient.jpg)\n",
    "\n",
    "Before we understand how to resolve this problem, let's try to quickly recap what we learned about RNNs so far.\n",
    "\n",
    "|RNN Advantages|RNN Disadvantages|\n",
    "|---|---|\n",
    "|Capability of processing any length input<br>(Model size not affected with size of input)|Difficulty in capturing long distance dependencies|\n",
    "|Past information taken into account during computation|Relatively slow processing|\n",
    "|Weights are shared across time(BPTT)||\n",
    "\n",
    "\n",
    "**Solution to Vanishing and Exploding Gradients Problem in RNN:**\n",
    "\n",
    "1. Smart Weight Initialization:\n",
    "\n",
    "When we initialize weights too small(<<1), it leads to vanishing gradient\n",
    "When we initialize weights too large(>>1), it leads to exploding gradient \n",
    "\n",
    "If we initialize weights randomly from a uniform distribution, it leads to lesser likelihood of gradient instability. Another way is to initialize weights randomly from a normal distribution. \n",
    "\n",
    "2. Better Activation Functions:\n",
    "\n",
    "Using activation functions like Relu, Leaky Relu(as opposed to Sigmoid, Tanh function) helps avoid exploding and vanishing gradients issue since it outputs a constant gradient of 1 for all the inputs > 0. This makes the neural net learn faster and speeds up the convergence of the training process.\n",
    "\n",
    "3. LSTM:\n",
    "\n",
    "An RNN overwrites its memory at each time step in a relatively uncontrolled fashion, an LSTM on the other hand transforms its memory by using specific learning mechanisms that help it keep long term track of information. Let's look at in detail in the next chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Introduction to LSTM\n",
    "\n",
    "Let's go back to the Tom Cruise movie example, you would like to have the model learn about the world in a controlled way with the following mechanisms:\n",
    "\n",
    "- Need of a forget mechanism:\n",
    "\n",
    "If the movie scene ends, the model should not keep track(forget) any scene-specific information(time of day, location, etc). \n",
    "\n",
    "At the same time, if a Tom kills someone in the scene, it should remember that the other character is no longer alive. \n",
    "\n",
    "\n",
    "- Need of a saving mechanism:\n",
    "\n",
    "Another important mechanism is with respect to input processing. When a new input comes in, it needs to identify which part of scene to save. Maybe there's a scene with Tom telling a joke to his coworker(You don't need that information saved)\n",
    "\n",
    "In summary what we need is when new a input comes in, the model first needs to forget any previous long-term information it decides is not required. After that it learns which parts of the new input are worth using, and stores them into its long-term memory.\n",
    "\n",
    "- Selective long term memory\n",
    "\n",
    "Think of the forget/save mechanism as maintaining a library(of information). This library model is useless though if it doesn't know which parts of its long-term memory is immediately needed. \n",
    "\n",
    "For example, Tom's family details(Is he married? How many kids?) may be a useful piece of information to keep in the long term but is probably irrelevant if Tom is not in the current scene. So instead of using the full long-term memory for every scene, it needs to learn which parts to focus on.\n",
    "\n",
    "\n",
    "The three features is what is more or less there in a variant of RNN called 'Long Short-Term Memory networks'\n",
    "\n",
    "**Long Short-Term Memory Networks**\n",
    "\n",
    "LSTM networks are explicitly designed to avoid the long-term dependency problem that RNNs have. \n",
    "\n",
    "``\n",
    "Remembering information for long periods of time is practically their default behavior, not something they struggle to learn``\n",
    "\n",
    "***Source:***[Colah Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "As mentioned before An RNN overwrites its memory at each time step in a relatively uncontrolled fashion, an LSTM on the other hand transforms its memory by using specific learning mechanisms that help it keep long term track of information.\n",
    "\n",
    "\n",
    "Let's try to understand LSTM a bit more formally\n",
    "\n",
    "\n",
    "A crucial addition in LSTM is that the weight on the self-loop is conditioned on the context, rather than ﬁxed. By converting the weight of this self-loop controlled by another hidden unit(also called gated), the time scale of integration can be changed dynamically. \n",
    "\n",
    "\n",
    "An LSTM consist of three gates : input, forget, and output gates.\n",
    "\n",
    "Following is the LSTM circuit image:\n",
    "\n",
    "![](final_images/lstm_1.jpg)\n",
    "\n",
    "The image looks a little daunting now. Let's try to understand it better\n",
    "\n",
    "At time t, we receive a new input $x_t$. \n",
    "\n",
    "Let's start with modification to long-term memory, more specifically working memory(i.e. selective memory we talked about in the previous step).\n",
    "\n",
    "\n",
    "***Note:*** Long term memory is referred to as cell state($C_t$) and working memory is referred to as hidden state($h_t$).\n",
    "\n",
    "\n",
    "***\n",
    "***Step 1:***\n",
    "\n",
    "Identify which pieces of working memory(or hidden state $h_t$) to continue remembering and which to discard. This leads to creation of a forget gate(**$f_t$**)\n",
    "\n",
    "***Note:*** The term forget gate is bit of a misnomer because here by forgeting we are infact remembering\n",
    "\n",
    "Alternatively you can write,\n",
    "\n",
    "**${f_t}$ = Function of new input($x_t$) and previous working memory($h_{t-1}$)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "***\n",
    "***Step 2:***\n",
    "\n",
    "Compute the new information we can learn from the present input $x_t$ and store it temporaily in candidate memory\n",
    "\n",
    "Alternatively you can write,\n",
    "\n",
    "**$\\text{candidate_mem}$= Function of new input($x_t$) and long term memory($C_t$)**\n",
    "\n",
    "***\n",
    "***Step 3:*** \n",
    "\n",
    "Before we add the candidate memory, we want to learn which parts of it are actually worth using and saving. This gate where you are \"saving\" is the input gate($i_t$)\n",
    "\n",
    "Alternatively you can write,\n",
    "\n",
    "**${i_t}$=  Function of new input($x_t$) and  previous working memory($h_{t-1}$)**\n",
    "\n",
    "For e.g. In weather forecasting, you should take information about tomorrow's weather, but probably ignore the journalist speculations about the same.\n",
    "\n",
    "***Note:*** This is similar to remember gate but using different weight matrices.\n",
    "\n",
    "***Step 4:***\n",
    "\n",
    "Combining all these steps, we get the actual long-term memory or cell state($C_t$)\n",
    "\n",
    "Alternatively you can write,\n",
    "\n",
    "**${C_t}$= Function of previous long term memory(forget gate + candidate memory)and new long term memory candidate(input gate + candidate memory)**  \n",
    "\n",
    "In this step what we are effectively doing is updating long-term memory by forgetting memories we don't need further and saving useful pieces from new input.\n",
    "\n",
    "***\n",
    "***Step 5:***\n",
    "\n",
    "Update the working memory by focusing certain parts of long-term memory that will be immediately useful. This results in output gate($o_t$) \n",
    "\n",
    "**${o_t}$= Function of current input($x_t$) and previous working memory($h_{t-1}$)**\n",
    "\n",
    "\n",
    "Our new hidden state($h_t$) then becomes:\n",
    "\n",
    "**${h_t}$= function of output gate($o_t$) and current long term memory($C_t$).**\n",
    "\n",
    "\n",
    "\n",
    "****\n",
    "### Dive Deeper(Optional)\n",
    "\n",
    "Let's look at the LSTM arch. again and understand the gates.\n",
    "\n",
    "![](final_images/lstm_1.jpg)\n",
    "\n",
    "Following are the mathematical formulae of the gates we discussed:\n",
    "\n",
    "**Cell state($C_t$):**\n",
    "\n",
    "$\\widetilde{C_t}= tanh(W_c.[h_{t-1},x_t] + b_C)$\n",
    "\n",
    "$C_t= f_t*C_{t-1} + i_t*\\widetilde{C_t}$\n",
    "\n",
    "\n",
    "**Hidden state($h_t$):** \n",
    "\n",
    "$h_t=o_t*tanh(C_t)$\n",
    "\n",
    "\n",
    "**Forget gate($f_t$):**\n",
    "\n",
    "$f_t= \\sigma(W_f.[h_{t-1},x_t] + b_f)$\n",
    "\n",
    "**Input gate($i_t$):** \n",
    "\n",
    "$i_t= \\sigma(W_i.[h_{t-1},x_t]+ b_i)$\n",
    "\n",
    "**Output gate($o_t$):**\n",
    "\n",
    "$o_t= \\sigma(W_o[h_{t-1},x_t]+ b_o)$\n",
    "\n",
    "**End of Dive Deeper(Optional)**\n",
    "****\n",
    "\n",
    "To summarize, following is the comparision between LSTM and RNN:\n",
    "\n",
    "**RNN:**\n",
    "\n",
    "Updation of hidden gate.\n",
    "\n",
    "**LSTM:**\n",
    "\n",
    "Updation of cell state, hidden state\n",
    "\n",
    "Creation of forget gate, input gate, ouput gate( and candidate memory temporarily)\n",
    "\n",
    "\n",
    "***If you want to understand the process better, you can check out this [fantastic blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) explaining the underlying mathematics.***\n",
    "\n",
    "Let's now get our hands dirty with actual LSTM coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/5\n",
      "7185/7185 [==============================] - 38s 5ms/step - loss: 3.1370 - acc: 0.2924 - val_loss: 2.4465 - val_acc: 0.3450\n",
      "Epoch 2/5\n",
      "7185/7185 [==============================] - 34s 5ms/step - loss: 2.4743 - acc: 0.3608 - val_loss: 2.1608 - val_acc: 0.4791\n",
      "Epoch 3/5\n",
      "7185/7185 [==============================] - 36s 5ms/step - loss: 2.1263 - acc: 0.4427 - val_loss: 1.9238 - val_acc: 0.4942\n",
      "Epoch 4/5\n",
      "7185/7185 [==============================] - 33s 5ms/step - loss: 1.8841 - acc: 0.4955 - val_loss: 1.7690 - val_acc: 0.5181\n",
      "Epoch 5/5\n",
      "7185/7185 [==============================] - 38s 5ms/step - loss: 1.8165 - acc: 0.5265 - val_loss: 1.7022 - val_acc: 0.5626\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(partial_x_train, partial_y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_split=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 LSTM variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you understood LSTM so far(***If you haven't then by all means read again. LSTM is both the most complex as well most pivotal concept that you are going to encounter in NLP using DL***), you might think that LSTM architecture could be optimised. Thankfully, that's what leading researches also ended up thinking which lead to many optimised LSTM architectures.\n",
    "\n",
    "Following are the two variations of the same: \n",
    "\n",
    "\n",
    "\n",
    "### 1. GRU\n",
    "\n",
    "One thing that seems odd in LSTM is the use of both long-term and working memories. That leads to oddity of why to use both input gates and forget gates.\n",
    "The tackling of this oddity led to the conception of LSTM variant called Gated Recurent Unit(GRU).\n",
    "\n",
    "Following is the circuit image of GRU:\n",
    "![](final_images/gru_1.png)\n",
    "\n",
    "\n",
    "A GRU has two gates, a reset gate `r` and an update gate `z`.  \n",
    "\n",
    "\n",
    "**Update gate(z):**\n",
    "\n",
    "We combine forget and input gates to create a single 'update' gate. Instead of separately learning what to forget and new information to add, we do that together. \n",
    "\n",
    "**Reset gate(r):**\n",
    "\n",
    "The reset gate is another gate that is used to decide how much past information to forget. \n",
    "\n",
    "\n",
    "Interestingly if you set the reset gate value to 1 and  update gate to 0, you will end up with a vanilla RNN model. Following are GRU's key differences when compared with LSTM:\n",
    "\n",
    "\n",
    "- A GRU has 2 gates, an LSTM has 3 gates.\n",
    "\n",
    "- GRUs don’t possess internal memory($C_t$) \n",
    "\n",
    "- GRUs don’t have the output gate like LSTMs.\n",
    "\n",
    "- Owing to fewer operations; they are faster to train & need less data to generalize than LSTMs. Conversely in cases with enough data, LSTMs with their greater flexibility lead to better results.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "### Dive Deeper(Optional):\n",
    "\n",
    "Following are the equations of GRU:\n",
    "\n",
    "$z_t= \\sigma(W_z.[h_{t-1},x_t])$\n",
    "\n",
    "$r_t= \\sigma(W_r.[h_{t-1},x_t])$\n",
    "\n",
    "$\\widetilde{h_t}= tanh(W.[r_t*h_{t-1},x_t])$\n",
    "\n",
    "${h_t}= (1-z_t)*h_{t-1} + z_t*\\widetilde{h_t}$\n",
    "\n",
    "\n",
    "Following is a side by side operation flow of LSTM and GRU\n",
    "\n",
    "|LSTM|GRU|\n",
    "|-----|-----|\n",
    "|![](final_images/lstm.gif) |![](final_images/gru.gif)|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**End of Dive Deeper(Optional)**\n",
    "***\n",
    "\n",
    "### 2. Bi-Directional LSTM\n",
    "\n",
    "Another variation of LSTM is the bidirectional LSTM. \n",
    "\n",
    "As the term `bidirectional` suggests, you try to predict data based on flow from both sides.\n",
    "\n",
    "Take the example of the following sentences:\n",
    "\n",
    "\"The man went to the local pool. He swam for a while and then got out of the pool\"\n",
    "\n",
    "Let's say we want to predict the word 'pool' in the sentence:\n",
    "\n",
    "Unidirectional LSTM only preserves information of the past because it has only seen inputs from the past.\n",
    "\n",
    "On a surface level what a unidirectional LSTM will see is the following:\n",
    "\n",
    "The man went to the local `_____`\n",
    "\n",
    "With bidirectional LSTM we will be able to see information further down the road for example:\n",
    "\n",
    "Forward LSTM:\n",
    "\n",
    "The man went to ...\n",
    "\n",
    "Backward LSTM:\n",
    "\n",
    "...swam and then got out of pool.\n",
    "\n",
    "Using the information from the future makes it easier for the network to understand what the next word is.\n",
    "\n",
    "\n",
    "![](final_images/bilstm.jpg)\n",
    "\n",
    "The working of BiLSTM(or any BiRNN for that matter) is as follows:\n",
    "\n",
    "- Separate the neurons of a regular LSTM into two directions, one for positive time direction (forward states: seeing the input vectors in correct order), and another for negative time direction (backward states: seeing the input vectors in reverse order). \n",
    "\n",
    "\n",
    "- BiLSTMs are trained using similar algorithms to LSTMs, because the two directional neurons do not have any interactions. Just that forward propagation is applied twice , one for the forward cells and one for the backward cells\n",
    "\n",
    "\n",
    "- When weight updation via back-propagation is applied, additional processes are required because updating input and output layers cannot be done simultaneously.\n",
    "\n",
    "\n",
    "- During forward pass, the model passes forward states and backward states first and then output neurons. During backward pass, the model passes output neurons first and then forward states and backward states. The weights are updated after forward and backward passes are completed. \n",
    "\n",
    "Applications of Bidirectional LSTM include :\n",
    "\n",
    "- POS tagging\n",
    "- Named Entity Recognition\n",
    "- Speech Recognition \n",
    "- Machine Translation\n",
    "\n",
    "It's more effective and more complex than unidirectional LSTM arch. Though it should be used depending upon the required application (For e.g Bi-Directional LSTMs cannot be used where we don't have access to the full sequence like real time translation)\n",
    "\n",
    "You can understand in detail about Bidirectional LSTM in this [video](https://www.youtube.com/watch?v=bTXGpATdKRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7185 samples, validate on 1797 samples\n",
      "Epoch 1/5\n",
      "7185/7185 [==============================] - 154s 21ms/step - loss: 2.9622 - acc: 0.3422 - val_loss: 2.3699 - val_acc: 0.3450\n",
      "Epoch 2/5\n",
      "7185/7185 [==============================] - 145s 20ms/step - loss: 2.1405 - acc: 0.4455 - val_loss: 1.9736 - val_acc: 0.4880\n",
      "Epoch 3/5\n",
      "7185/7185 [==============================] - 149s 21ms/step - loss: 1.9617 - acc: 0.4731 - val_loss: 1.8977 - val_acc: 0.4769\n",
      "Epoch 4/5\n",
      "7185/7185 [==============================] - 145s 20ms/step - loss: 1.7534 - acc: 0.5301 - val_loss: 1.7594 - val_acc: 0.5403\n",
      "Epoch 5/5\n",
      "7185/7185 [==============================] - 133s 18ms/step - loss: 1.6172 - acc: 0.5844 - val_loss: 1.6970 - val_acc: 0.5698\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, GRU, Bidirectional\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100))\n",
    "# model.add(SimpleRNN(32))\n",
    "# model.add(SimpleRNN(32, return_sequences=True))\n",
    "# model.add(SimpleRNN(32, return_sequences=True))\n",
    "# model.add(SimpleRNN(32, return_sequences=True))\n",
    "# model.add(LSTM(32))\n",
    "# model.add(GRU(32))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "# model.add(Dropout(0.5))\n",
    "model.add(Dense(46, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(partial_x_train, partial_y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_split=.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# -----------------------------------------------\n",
    "# END OF CONCEPT\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
